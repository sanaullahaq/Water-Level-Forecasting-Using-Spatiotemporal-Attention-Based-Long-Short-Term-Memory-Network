{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nF5vWMZFE38r",
        "ud2VXWxviTZd",
        "QYSiD6SIRfYo",
        "AEPN5VUhDjSO",
        "EySn84hFEGF6",
        "P4FjDE54Dwbd",
        "zmFfX9oWDz9-",
        "IweLh4SwD_EH",
        "_lj7kGSXEhwJ",
        "re4exhU4pbqG",
        "OQPS5sqToYX8",
        "MwTX4F5isvrO",
        "9NKyGlzvxyOB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF5vWMZFE38r"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPY_pq8aE3Zg"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCyL6sYyEv3G"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTD2F9V_EvdX"
      },
      "source": [
        "# Hyperparameters for LSTM\n",
        "\n",
        "IN_DIM = 2 \n",
        "SEQUENCE_LENGTH = 1  # Lookback window \n",
        "LSTM_IN_DIM = int(IN_DIM / SEQUENCE_LENGTH)  # The input size of LSTM is equal to the total variable length/time series length\n",
        "LSTM_HIDDEN_DIM = 300  # LSTM Hidden state\n",
        "OUT_DIM = 1  # Output size\n",
        "LEARNING_RATE = 0.01  # learning rate\n",
        "WEIGHT_DECAY = 1e-6  # L2 Penalty\n",
        "BATCH_SIZE = 50  # batch size\n",
        "EPOCHS = 2000  # epoch Size\n",
        "TRAIN_PER = 0.80  # Proportion of training set\n",
        "VALI_PER = 0.0  # Proportion of validation set\n",
        "USE_GPU = False"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh9KzIPdk_pR"
      },
      "source": [
        "# Hyperparameters for ANN\n",
        "num_epochs = 2000\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "num_layers = 1\n",
        "num_classes = 1\n",
        "#net = ANN(input_size,20,num_classes)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud2VXWxviTZd"
      },
      "source": [
        "# DATASET PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjNeJALkRfYi"
      },
      "source": [
        "## Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YHJMI4YK8Ede",
        "outputId": "b8fa3f55-1842-436f-92da-aeb3c3b47ca5"
      },
      "source": [
        "EXCEL_PATH = 'Data Mining.xlsx'\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "df = pd.read_excel(EXCEL_PATH)\n",
        "df"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>SW273</th>\n",
              "      <th>SW267</th>\n",
              "      <th>SW269</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1979-06-01</td>\n",
              "      <td>2.60</td>\n",
              "      <td>5.52</td>\n",
              "      <td>5.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1979-06-02</td>\n",
              "      <td>2.43</td>\n",
              "      <td>5.30</td>\n",
              "      <td>5.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1979-06-03</td>\n",
              "      <td>2.40</td>\n",
              "      <td>5.07</td>\n",
              "      <td>5.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1979-06-04</td>\n",
              "      <td>2.29</td>\n",
              "      <td>4.87</td>\n",
              "      <td>4.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1979-06-05</td>\n",
              "      <td>2.20</td>\n",
              "      <td>4.69</td>\n",
              "      <td>4.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>2009-10-27</td>\n",
              "      <td>3.50</td>\n",
              "      <td>6.13</td>\n",
              "      <td>5.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5011</th>\n",
              "      <td>2009-10-28</td>\n",
              "      <td>3.34</td>\n",
              "      <td>5.97</td>\n",
              "      <td>5.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5012</th>\n",
              "      <td>2009-10-29</td>\n",
              "      <td>3.20</td>\n",
              "      <td>5.82</td>\n",
              "      <td>5.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5013</th>\n",
              "      <td>2009-10-30</td>\n",
              "      <td>3.12</td>\n",
              "      <td>5.70</td>\n",
              "      <td>5.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5014</th>\n",
              "      <td>2009-10-31</td>\n",
              "      <td>3.06</td>\n",
              "      <td>5.61</td>\n",
              "      <td>5.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5015 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           DATE  SW273  SW267  SW269\n",
              "0    1979-06-01   2.60   5.52   5.42\n",
              "1    1979-06-02   2.43   5.30   5.24\n",
              "2    1979-06-03   2.40   5.07   5.09\n",
              "3    1979-06-04   2.29   4.87   4.94\n",
              "4    1979-06-05   2.20   4.69   4.79\n",
              "...         ...    ...    ...    ...\n",
              "5010 2009-10-27   3.50   6.13   5.59\n",
              "5011 2009-10-28   3.34   5.97   5.50\n",
              "5012 2009-10-29   3.20   5.82   5.42\n",
              "5013 2009-10-30   3.12   5.70   5.36\n",
              "5014 2009-10-31   3.06   5.61   5.30\n",
              "\n",
              "[5015 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smNWw5qkRfYn"
      },
      "source": [
        "## Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-_2WjhM-dAo"
      },
      "source": [
        "class FloodDataset(Dataset):\n",
        "  \"\"\"Custom Dataset class to work with FloodDataset\"\"\"\n",
        "  def __init__(self, excel_path, train, items=None):\n",
        "    df = pd.read_excel(excel_path)\n",
        "    \n",
        "    if items and train==True:\n",
        "      df = df[:items]\n",
        "\n",
        "    elif items and train==False:\n",
        "      df = df[-items:]\n",
        "\n",
        "    df['index'] = range(0, len(df))\n",
        "    df = df.set_index('index')\n",
        "    \n",
        "    self.excel_path = excel_path\n",
        "    self.features = df[['SW273','SW269']]\n",
        "    self.targets = df[['SW267']]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return torch.tensor(self.features.iloc[index], dtype=torch.float32), torch.tensor(self.targets.iloc[index], dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpGzvYqxn4Tq"
      },
      "source": [
        "# df[['SW267', 'SW269']].iloc[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f_i4Yj4v8b--",
        "outputId": "1fb7ebb2-a659-44ed-8a12-2250ca8b1eba"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SW273</th>\n",
              "      <th>SW267</th>\n",
              "      <th>SW269</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5015.000000</td>\n",
              "      <td>5015.000000</td>\n",
              "      <td>5015.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.971986</td>\n",
              "      <td>8.797886</td>\n",
              "      <td>7.088640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.248850</td>\n",
              "      <td>2.063672</td>\n",
              "      <td>1.233145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.670000</td>\n",
              "      <td>3.140000</td>\n",
              "      <td>3.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.050000</td>\n",
              "      <td>7.730000</td>\n",
              "      <td>6.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.300000</td>\n",
              "      <td>9.510000</td>\n",
              "      <td>7.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.900000</td>\n",
              "      <td>10.340000</td>\n",
              "      <td>7.970000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.780000</td>\n",
              "      <td>12.440000</td>\n",
              "      <td>9.750000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             SW273        SW267        SW269\n",
              "count  5015.000000  5015.000000  5015.000000\n",
              "mean      4.971986     8.797886     7.088640\n",
              "std       1.248850     2.063672     1.233145\n",
              "min       1.670000     3.140000     3.180000\n",
              "25%       4.050000     7.730000     6.460000\n",
              "50%       5.300000     9.510000     7.430000\n",
              "75%       5.900000    10.340000     7.970000\n",
              "max       7.780000    12.440000     9.750000"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvukiO848tdd",
        "outputId": "35ba4129-1b48-41f0-ab15-2ce2b6aaae6f"
      },
      "source": [
        "df.SW273.isnull().sum(), df.SW267.isnull().sum(), df.SW269.isnull().sum()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYSiD6SIRfYo"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Enkj1QBNtX",
        "outputId": "df51f169-9d9f-480b-9269-9dff2a9ee759"
      },
      "source": [
        "train_dataset = FloodDataset(excel_path=EXCEL_PATH,\n",
        "                             train=True,\n",
        "                             items = 4011)\n",
        "test_dataset = FloodDataset(excel_path=EXCEL_PATH,\n",
        "                             train=False,\n",
        "                             items = 1003)\n",
        "print(f'Length Train Dataset: {len(train_dataset)} \\nLength of Test Dataset: {len(test_dataset)}')\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size = 1000,\n",
        "                          shuffle=False,\n",
        "                          num_workers = 0)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size = 1003,\n",
        "                         shuffle=False,\n",
        "                         num_workers=0)\n",
        "\n",
        "print(f'Length Train Loader: {len(train_dataloader)} \\nLength of Test Loader: {len(test_dataloader)}')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length Train Dataset: 4011 \n",
            "Length of Test Dataset: 1003\n",
            "Length Train Loader: 5 \n",
            "Length of Test Loader: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmmmyfKmmYrh"
      },
      "source": [
        "# train_dataset.input, train_dataset.output\n",
        "# test_dataset.input, test_dataset.output"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4oTiBtAGNYA"
      },
      "source": [
        "# sample = next(iter(train_loader))\n",
        "# print(len(sample))\n",
        "# inp, out = sample\n",
        "# print(inp.shape, out.shape)\n",
        "# type(inp) ,type(out)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEPN5VUhDjSO"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EySn84hFEGF6"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jlN0vlYEHrF"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 sequence_length,\n",
        "                 lstm_in_dim,\n",
        "                 lstm_hidden_dim,\n",
        "                 out_dim,\n",
        "                 use_gpu=False):\n",
        "\n",
        "        super(LSTM,self).__init__()\n",
        "\n",
        "        # Parameter import part\n",
        "        self.in_dim = in_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.lstm_in_dim = lstm_in_dim\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # batch_norm layer\n",
        "        self.batch_norm = nn.BatchNorm1d(in_dim)\n",
        "        \n",
        "        # input layer\n",
        "        self.layer_in = nn.Linear(in_dim, in_dim)\n",
        "        \n",
        "        # lstmcell \n",
        "        self.lstmcell = nn.LSTMCell(lstm_in_dim, lstm_hidden_dim)\n",
        "         \n",
        "        # output layer, 1 × lstm_hiddendim -> 1 × 1\n",
        "        self.layer_out = nn.Linear(lstm_hidden_dim, out_dim,bias=False)\n",
        "        \n",
        "        # activate functions\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward (self,input):\n",
        "        out = self.batch_norm(input)\n",
        "        out = self.layer_in(out)\n",
        "\n",
        "        # Initialize hidden state and memory cell state\n",
        "        h_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim) # batch, hideden_size\n",
        "        c_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim) # batch, hideden_size\n",
        "       \n",
        "        for i in range(self.sequence_length):\n",
        "            x_t = out[:,i*self.lstm_in_dim:(i+1)*(self.lstm_in_dim)]\n",
        "            h_t,c_t = self.lstmcell(x_t,(h_t_1,c_t_1)) \n",
        "            h_t_1,c_t_1 = h_t,c_t\n",
        "        out = self.layer_out(h_t)\n",
        "        return out"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4FjDE54Dwbd"
      },
      "source": [
        "## Spatio-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgJDvmgdDi47"
      },
      "source": [
        "class SA_LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, sequence_length, lstm_in_dim, lstm_hidden_dim, out_dim, use_gpu=False):\n",
        "        super(SA_LSTM,self).__init__()\n",
        " \n",
        "        self.in_dim = in_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.lstm_in_dim = lstm_in_dim\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        self.batch_norm = nn.BatchNorm1d(in_dim)\n",
        "\n",
        "        self.layer_in = nn.Linear(in_dim, in_dim,bias=False)\n",
        "\n",
        "\n",
        "        self.S_A = nn.Linear(lstm_in_dim, lstm_in_dim)\n",
        "\n",
        "        self.lstmcell = nn.LSTMCell(lstm_in_dim, lstm_hidden_dim)\n",
        "\n",
        "        # # output layer, 1 × lstm_hiddendim -> 1 × 1\n",
        "        self.layer_out = nn.Linear(lstm_hidden_dim, out_dim,bias=False)\n",
        "         \n",
        "        # activate functions\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward (self,input):\n",
        "\n",
        "        out = self.batch_norm(input)\n",
        "        out = self.layer_in(out)\n",
        "        h_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim) \n",
        "        c_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim)\n",
        "      \n",
        "        for i in range(self.sequence_length):\n",
        "            x_t = out[       : ,       i*self.lstm_in_dim : (i+1)*(self.lstm_in_dim)]\n",
        "            alpha_t =  self.sigmoid(self.S_A(x_t))\n",
        "            alpha_t = self.softmax(alpha_t)\n",
        "            h_t,c_t = self.lstmcell(x_t*alpha_t,(h_t_1,c_t_1))\n",
        "            h_t_1,c_t_1 = h_t,c_t\n",
        "        out = self.layer_out(h_t)\n",
        "        return out"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmFfX9oWDz9-"
      },
      "source": [
        "## Temporal-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pcDTW4tD5Zz"
      },
      "source": [
        "class TA_LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, sequence_length, lstm_in_dim, lstm_hidden_dim, out_dim, use_gpu=False):\n",
        "        super(TA_LSTM,self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.lstm_in_dim = lstm_in_dim\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        self.batch_norm = nn.BatchNorm1d(in_dim)\n",
        "        \n",
        "        self.layer_in = nn.Linear(in_dim, in_dim,bias=False)\n",
        "        # lstmcell \n",
        "        self.lstmcell = nn.LSTMCell(lstm_in_dim, lstm_hidden_dim)\n",
        "        # temporal attention module, \n",
        "        self.T_A = nn.Linear(sequence_length*lstm_hidden_dim, sequence_length)\n",
        "        # # output layer, \n",
        "        self.layer_out = nn.Linear(lstm_hidden_dim, out_dim,bias=False)\n",
        "        # activate functions\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward (self,input):\n",
        "\n",
        "        out = self.batch_norm(input)\n",
        "        out = self.layer_in(out)\n",
        "        h_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim) # batch, hidden_size\n",
        "        c_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim) # batch, hidden_size\n",
        "      \n",
        "        h_list = []\n",
        "\n",
        "        for i in range(self.sequence_length):\n",
        "            \n",
        "            x_t = out[:,i*self.lstm_in_dim:(i+1)*(self.lstm_in_dim)]\n",
        "           \n",
        "            h_t,c_t = self.lstmcell(x_t,(h_t_1,c_t_1)) \n",
        "            \n",
        "            h_list.append(h_t)\n",
        "\n",
        "            h_t_1,c_t_1 = h_t,c_t\n",
        "        \n",
        "        total_ht = h_list[0]\n",
        "        for i in range(1,len(h_list)):\n",
        "            total_ht = torch.cat((total_ht,h_list[i]),1)    \n",
        "        \n",
        "        beta_t =  self.relu(self.T_A(total_ht))\n",
        "        beta_t = self.softmax(beta_t)\n",
        "        \n",
        "        out = torch.zeros(out.size(0), self.lstm_hidden_dim)\n",
        "        # print(h_list[i].size(),beta_t[:,1].size())\n",
        "\n",
        "        for i in range(len(h_list)):\n",
        "                      \n",
        "            out = out + h_list[i]*beta_t[:,i].reshape(out.size(0),1)\n",
        "\n",
        "        out = self.layer_out(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IweLh4SwD_EH"
      },
      "source": [
        "## Spatio-Temporal LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCp2PYtYETPu"
      },
      "source": [
        "class STA_LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 sequence_length,\n",
        "                 lstm_in_dim,\n",
        "                 lstm_hidden_dim,\n",
        "                 out_dim,\n",
        "                 use_gpu=False):\n",
        "\n",
        "        super(STA_LSTM,self).__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "        self.lstm_in_dim = lstm_in_dim\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # batch_norm layer\n",
        "        self.batch_norm = nn.BatchNorm1d(in_dim)\n",
        "        \n",
        "        # input layer\n",
        "        self.layer_in = nn.Linear(in_dim, in_dim,bias=False)\n",
        "\n",
        "        # spatial attention module\n",
        "        self.S_A = nn.Linear(lstm_in_dim, lstm_in_dim)\n",
        "        \n",
        "        # lstmcell \n",
        "        self.lstmcell = nn.LSTMCell(lstm_in_dim, lstm_hidden_dim)\n",
        "        \n",
        "        # temporal atteention module\n",
        "        self.T_A = nn.Linear(sequence_length*lstm_hidden_dim, sequence_length)\n",
        "        \n",
        "        # # output layer, 1 × lstm_hiddendim -> 1 × 1\n",
        "        self.layer_out = nn.Linear(lstm_hidden_dim, out_dim,bias=False)\n",
        "         \n",
        "        # activate functions\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward (self,input):\n",
        "\n",
        "        out = self.batch_norm(input)\n",
        "\n",
        "        out = self.layer_in(out)\n",
        "         \n",
        "        h_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim) # batch, hidden_size\n",
        "        c_t_1 = torch.zeros(out.size(0), self.lstm_hidden_dim) # batch, hidden_size\n",
        "      \n",
        "        h_list = []\n",
        "\n",
        "        for i in range(self.sequence_length):\n",
        "            \n",
        "            x_t = out[: ,  i*self.lstm_in_dim  :      (i+1)*(self.lstm_in_dim)]\n",
        "           \n",
        "            alpha_t =  self.sigmoid(self.S_A(x_t))\n",
        "\n",
        "            alpha_t = self.softmax(alpha_t)\n",
        "            # print(alpha_t)\n",
        "\n",
        "            h_t,c_t = self.lstmcell(x_t*alpha_t,(h_t_1,c_t_1)) \n",
        "            \n",
        "            h_list.append(h_t)\n",
        "\n",
        "            h_t_1,c_t_1 = h_t,c_t\n",
        "        \n",
        "        total_ht = h_list[0]\n",
        "        for i in range(1,len(h_list)):\n",
        "            total_ht = torch.cat((total_ht,h_list[i]),1)    \n",
        "        \n",
        "        beta_t =  self.relu(self.T_A(total_ht))\n",
        "        beta_t = self.softmax(beta_t)\n",
        "        # print(beta_t)\n",
        "        out = torch.zeros(out.size(0), self.lstm_hidden_dim)\n",
        "        # print(h_list[i].size(),beta_t[:,1].size())\n",
        "\n",
        "        for i in range(len(h_list)):\n",
        "                      \n",
        "            out = out + h_list[i]*beta_t[:,i].reshape(out.size(0),1)\n",
        "\n",
        "        out = self.layer_out(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lj7kGSXEhwJ"
      },
      "source": [
        "## ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T0-FmT8Eg19"
      },
      "source": [
        "class ANN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(ANN, self).__init__()\n",
        "    self.fc1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.output_layer = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "#     x = x.reshape(x.size(0), 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.output_layer(x)\n",
        "    return x"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsvDAj2gLJch"
      },
      "source": [
        "# **Model Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENKy1N7YHa06",
        "outputId": "c5968da6-38cf-406d-ea00-4ffd760ce631"
      },
      "source": [
        "net= ANN(IN_DIM,5,OUT_DIM)\n",
        "net = STA_LSTM(IN_DIM, SEQUENCE_LENGTH, LSTM_IN_DIM, LSTM_HIDDEN_DIM, OUT_DIM, USE_GPU)\n",
        "print('The network model is ready')\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "adjust_lr = optim.lr_scheduler.MultiStepLR(optimizer,  milestones=[i * 10 for i in range(EPOCHES // 10)], gamma=0.5)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The network model is ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re4exhU4pbqG"
      },
      "source": [
        "# Evaluation Metrics Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQPS5sqToYX8"
      },
      "source": [
        "### Evaluation Metric Classes(RMSE, R2)\n",
        "These cells are not being used at the moment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxU1aslDoYX9"
      },
      "source": [
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "    def forward(self,yhat,y):\n",
        "        return torch.sqrt(self.mse(yhat,y))\n",
        "#criterion = RMSELoss()     #loss = criterion(yhat,y)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CeJGxMGoYX9"
      },
      "source": [
        "def r2_loss(output, target):\n",
        "    target_mean = torch.mean(target)\n",
        "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
        "    ss_res = torch.sum((target - output) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwTX4F5isvrO"
      },
      "source": [
        "### Loss and Calculate Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67E-hfFa7GZ"
      },
      "source": [
        "def Loss(preds, targets, train):\n",
        "    error = preds-targets\n",
        "    abs_error = torch.abs(error)\n",
        "    \n",
        "    mse = torch.sum(error**2)\n",
        "    rmse = torch.sqrt(mse/len(preds))\n",
        "    \n",
        "    if train==True:\n",
        "        return mse, rmse\n",
        "    else:\n",
        "        mae = torch.sum(abs_error)\n",
        "        mape = torch.sum(abs_error/targets)\n",
        "\n",
        "        targets_mean = torch.mean(targets)\n",
        "        preds_mean = torch.mean(preds)\n",
        "        r_square_numerator = torch.sum((targets-targets_mean)*(preds-preds_mean))**2\n",
        "        r_square_denominator = torch.sum((targets-targets_mean)**2)*torch.sum((preds-preds_mean)**2)\n",
        "        r_square = r_square_numerator / r_square_denominator\n",
        "\n",
        "\n",
        "        esd = torch.sum((abs_error - torch.mean(abs_error))**2)\n",
        "        return mse, rmse, mae, mape, r_square, esd"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWkYxt-JXBFY"
      },
      "source": [
        "def calculate_scores(model, data_loader):\n",
        "  criterion = Loss \n",
        "  train_examples = 0\n",
        "  total_mae = total_mse = total_mape = total_r_square = total_esd = 0.0\n",
        "\n",
        "  for batch, (features, targets) in enumerate(data_loader):\n",
        "    preds = model(features)\n",
        "    \n",
        "    mse, rmse, mae, mape, r_square, esd = criterion(preds, targets, train=False)\n",
        "    total_mae += mae\n",
        "    total_mse += mse\n",
        "    total_mape += mape\n",
        "    train_examples +=len(features)\n",
        "    total_r_square += r_square\n",
        "    total_esd += esd\n",
        "\n",
        "  total_RMSE_loss = format(torch.sqrt((total_mse)/train_examples), '.5f')\n",
        "  total_mse_loss = format(total_mse/train_examples, '.5f')\n",
        "  total_mae_loss = format(total_mae/train_examples, '.5f')\n",
        "  total_mape_loss = format(total_mape/train_examples, '.5f')\n",
        "  total_r_square = format(total_r_square, '.5f')\n",
        "  total_esd = format(torch.sqrt(esd/train_examples), '.5f')\n",
        "  \n",
        "\n",
        "  print(f'total_RMSE_loss: {total_RMSE_loss}\\ntotal_mse_loss: {total_mse_loss}\\ntotal_mae_loss: {total_mae_loss}\\ntotal_mape_loss: {total_mape_loss}\\ntotal_r_square:{total_r_square}\\ntotal_esd: {total_esd}')"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3DBW-TtILNA"
      },
      "source": [
        "# Training the Model Using Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtO4lN3iwDuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabb836a-17d0-433c-9554-f0e32052c35d"
      },
      "source": [
        "net.train()\n",
        "criterion = Loss\n",
        "EPOCHS= 2000        ####################################################\n",
        "for epoch in range(EPOCHS):\n",
        "  train_examples = 0\n",
        "  train_loss = 0\n",
        "\n",
        "  for batch ,(features, targets) in enumerate(train_dataloader):\n",
        "    preds = net(features)\n",
        "    # print(type(preds), preds.shape)\n",
        "    mse_loss, rmse_loss = criterion(preds, targets, train=True)\n",
        "    optimizer.zero_grad()\n",
        "    rmse_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_examples += len(features)\n",
        "    train_loss += mse_loss\n",
        "  \n",
        "  train_loss = torch.sqrt(train_loss/train_examples)\n",
        "  print(f'Epoch = {epoch+1} RMSE Training Loss = {format(train_loss, \".5f\")}')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch = 1 RMSE Training Loss = 8.96101\n",
            "Epoch = 2 RMSE Training Loss = 8.71332\n",
            "Epoch = 3 RMSE Training Loss = 8.27878\n",
            "Epoch = 4 RMSE Training Loss = 7.55949\n",
            "Epoch = 5 RMSE Training Loss = 6.45720\n",
            "Epoch = 6 RMSE Training Loss = 4.86439\n",
            "Epoch = 7 RMSE Training Loss = 2.69654\n",
            "Epoch = 8 RMSE Training Loss = 0.71238\n",
            "Epoch = 9 RMSE Training Loss = 1.58353\n",
            "Epoch = 10 RMSE Training Loss = 1.00365\n",
            "Epoch = 11 RMSE Training Loss = 0.89702\n",
            "Epoch = 12 RMSE Training Loss = 0.73853\n",
            "Epoch = 13 RMSE Training Loss = 0.62104\n",
            "Epoch = 14 RMSE Training Loss = 0.60274\n",
            "Epoch = 15 RMSE Training Loss = 0.54646\n",
            "Epoch = 16 RMSE Training Loss = 0.49247\n",
            "Epoch = 17 RMSE Training Loss = 0.55172\n",
            "Epoch = 18 RMSE Training Loss = 0.54146\n",
            "Epoch = 19 RMSE Training Loss = 0.52127\n",
            "Epoch = 20 RMSE Training Loss = 0.52361\n",
            "Epoch = 21 RMSE Training Loss = 0.52560\n",
            "Epoch = 22 RMSE Training Loss = 0.51445\n",
            "Epoch = 23 RMSE Training Loss = 0.51661\n",
            "Epoch = 24 RMSE Training Loss = 0.52505\n",
            "Epoch = 25 RMSE Training Loss = 0.52129\n",
            "Epoch = 26 RMSE Training Loss = 0.51961\n",
            "Epoch = 27 RMSE Training Loss = 0.52254\n",
            "Epoch = 28 RMSE Training Loss = 0.52131\n",
            "Epoch = 29 RMSE Training Loss = 0.52118\n",
            "Epoch = 30 RMSE Training Loss = 0.52390\n",
            "Epoch = 31 RMSE Training Loss = 0.52439\n",
            "Epoch = 32 RMSE Training Loss = 0.52439\n",
            "Epoch = 33 RMSE Training Loss = 0.52580\n",
            "Epoch = 34 RMSE Training Loss = 0.52662\n",
            "Epoch = 35 RMSE Training Loss = 0.52729\n",
            "Epoch = 36 RMSE Training Loss = 0.52870\n",
            "Epoch = 37 RMSE Training Loss = 0.52978\n",
            "Epoch = 38 RMSE Training Loss = 0.53061\n",
            "Epoch = 39 RMSE Training Loss = 0.53176\n",
            "Epoch = 40 RMSE Training Loss = 0.53285\n",
            "Epoch = 41 RMSE Training Loss = 0.53383\n",
            "Epoch = 42 RMSE Training Loss = 0.53494\n",
            "Epoch = 43 RMSE Training Loss = 0.53600\n",
            "Epoch = 44 RMSE Training Loss = 0.53695\n",
            "Epoch = 45 RMSE Training Loss = 0.53795\n",
            "Epoch = 46 RMSE Training Loss = 0.53891\n",
            "Epoch = 47 RMSE Training Loss = 0.53982\n",
            "Epoch = 48 RMSE Training Loss = 0.54072\n",
            "Epoch = 49 RMSE Training Loss = 0.54157\n",
            "Epoch = 50 RMSE Training Loss = 0.54237\n",
            "Epoch = 51 RMSE Training Loss = 0.54315\n",
            "Epoch = 52 RMSE Training Loss = 0.54389\n",
            "Epoch = 53 RMSE Training Loss = 0.54459\n",
            "Epoch = 54 RMSE Training Loss = 0.54525\n",
            "Epoch = 55 RMSE Training Loss = 0.54588\n",
            "Epoch = 56 RMSE Training Loss = 0.54647\n",
            "Epoch = 57 RMSE Training Loss = 0.54702\n",
            "Epoch = 58 RMSE Training Loss = 0.54754\n",
            "Epoch = 59 RMSE Training Loss = 0.54802\n",
            "Epoch = 60 RMSE Training Loss = 0.54848\n",
            "Epoch = 61 RMSE Training Loss = 0.54890\n",
            "Epoch = 62 RMSE Training Loss = 0.54929\n",
            "Epoch = 63 RMSE Training Loss = 0.54966\n",
            "Epoch = 64 RMSE Training Loss = 0.54999\n",
            "Epoch = 65 RMSE Training Loss = 0.55030\n",
            "Epoch = 66 RMSE Training Loss = 0.55059\n",
            "Epoch = 67 RMSE Training Loss = 0.55085\n",
            "Epoch = 68 RMSE Training Loss = 0.55109\n",
            "Epoch = 69 RMSE Training Loss = 0.55131\n",
            "Epoch = 70 RMSE Training Loss = 0.55151\n",
            "Epoch = 71 RMSE Training Loss = 0.55169\n",
            "Epoch = 72 RMSE Training Loss = 0.55185\n",
            "Epoch = 73 RMSE Training Loss = 0.55200\n",
            "Epoch = 74 RMSE Training Loss = 0.55213\n",
            "Epoch = 75 RMSE Training Loss = 0.55224\n",
            "Epoch = 76 RMSE Training Loss = 0.55234\n",
            "Epoch = 77 RMSE Training Loss = 0.55242\n",
            "Epoch = 78 RMSE Training Loss = 0.55249\n",
            "Epoch = 79 RMSE Training Loss = 0.55255\n",
            "Epoch = 80 RMSE Training Loss = 0.55260\n",
            "Epoch = 81 RMSE Training Loss = 0.55263\n",
            "Epoch = 82 RMSE Training Loss = 0.55266\n",
            "Epoch = 83 RMSE Training Loss = 0.55267\n",
            "Epoch = 84 RMSE Training Loss = 0.55268\n",
            "Epoch = 85 RMSE Training Loss = 0.55267\n",
            "Epoch = 86 RMSE Training Loss = 0.55266\n",
            "Epoch = 87 RMSE Training Loss = 0.55264\n",
            "Epoch = 88 RMSE Training Loss = 0.55261\n",
            "Epoch = 89 RMSE Training Loss = 0.55258\n",
            "Epoch = 90 RMSE Training Loss = 0.55253\n",
            "Epoch = 91 RMSE Training Loss = 0.55249\n",
            "Epoch = 92 RMSE Training Loss = 0.55243\n",
            "Epoch = 93 RMSE Training Loss = 0.55237\n",
            "Epoch = 94 RMSE Training Loss = 0.55230\n",
            "Epoch = 95 RMSE Training Loss = 0.55223\n",
            "Epoch = 96 RMSE Training Loss = 0.55215\n",
            "Epoch = 97 RMSE Training Loss = 0.55207\n",
            "Epoch = 98 RMSE Training Loss = 0.55198\n",
            "Epoch = 99 RMSE Training Loss = 0.55189\n",
            "Epoch = 100 RMSE Training Loss = 0.55180\n",
            "Epoch = 101 RMSE Training Loss = 0.55170\n",
            "Epoch = 102 RMSE Training Loss = 0.55159\n",
            "Epoch = 103 RMSE Training Loss = 0.55149\n",
            "Epoch = 104 RMSE Training Loss = 0.55137\n",
            "Epoch = 105 RMSE Training Loss = 0.55126\n",
            "Epoch = 106 RMSE Training Loss = 0.55114\n",
            "Epoch = 107 RMSE Training Loss = 0.55102\n",
            "Epoch = 108 RMSE Training Loss = 0.55090\n",
            "Epoch = 109 RMSE Training Loss = 0.55077\n",
            "Epoch = 110 RMSE Training Loss = 0.55065\n",
            "Epoch = 111 RMSE Training Loss = 0.55051\n",
            "Epoch = 112 RMSE Training Loss = 0.55038\n",
            "Epoch = 113 RMSE Training Loss = 0.55024\n",
            "Epoch = 114 RMSE Training Loss = 0.55011\n",
            "Epoch = 115 RMSE Training Loss = 0.54997\n",
            "Epoch = 116 RMSE Training Loss = 0.54982\n",
            "Epoch = 117 RMSE Training Loss = 0.54968\n",
            "Epoch = 118 RMSE Training Loss = 0.54953\n",
            "Epoch = 119 RMSE Training Loss = 0.54939\n",
            "Epoch = 120 RMSE Training Loss = 0.54924\n",
            "Epoch = 121 RMSE Training Loss = 0.54909\n",
            "Epoch = 122 RMSE Training Loss = 0.54893\n",
            "Epoch = 123 RMSE Training Loss = 0.54878\n",
            "Epoch = 124 RMSE Training Loss = 0.54862\n",
            "Epoch = 125 RMSE Training Loss = 0.54847\n",
            "Epoch = 126 RMSE Training Loss = 0.54831\n",
            "Epoch = 127 RMSE Training Loss = 0.54815\n",
            "Epoch = 128 RMSE Training Loss = 0.54799\n",
            "Epoch = 129 RMSE Training Loss = 0.54783\n",
            "Epoch = 130 RMSE Training Loss = 0.54766\n",
            "Epoch = 131 RMSE Training Loss = 0.54750\n",
            "Epoch = 132 RMSE Training Loss = 0.54733\n",
            "Epoch = 133 RMSE Training Loss = 0.54717\n",
            "Epoch = 134 RMSE Training Loss = 0.54700\n",
            "Epoch = 135 RMSE Training Loss = 0.54683\n",
            "Epoch = 136 RMSE Training Loss = 0.54667\n",
            "Epoch = 137 RMSE Training Loss = 0.54650\n",
            "Epoch = 138 RMSE Training Loss = 0.54633\n",
            "Epoch = 139 RMSE Training Loss = 0.54616\n",
            "Epoch = 140 RMSE Training Loss = 0.54599\n",
            "Epoch = 141 RMSE Training Loss = 0.54582\n",
            "Epoch = 142 RMSE Training Loss = 0.54564\n",
            "Epoch = 143 RMSE Training Loss = 0.54547\n",
            "Epoch = 144 RMSE Training Loss = 0.54530\n",
            "Epoch = 145 RMSE Training Loss = 0.54512\n",
            "Epoch = 146 RMSE Training Loss = 0.54495\n",
            "Epoch = 147 RMSE Training Loss = 0.54478\n",
            "Epoch = 148 RMSE Training Loss = 0.54460\n",
            "Epoch = 149 RMSE Training Loss = 0.54443\n",
            "Epoch = 150 RMSE Training Loss = 0.54425\n",
            "Epoch = 151 RMSE Training Loss = 0.54408\n",
            "Epoch = 152 RMSE Training Loss = 0.54390\n",
            "Epoch = 153 RMSE Training Loss = 0.54373\n",
            "Epoch = 154 RMSE Training Loss = 0.54355\n",
            "Epoch = 155 RMSE Training Loss = 0.54337\n",
            "Epoch = 156 RMSE Training Loss = 0.54320\n",
            "Epoch = 157 RMSE Training Loss = 0.54302\n",
            "Epoch = 158 RMSE Training Loss = 0.54284\n",
            "Epoch = 159 RMSE Training Loss = 0.54267\n",
            "Epoch = 160 RMSE Training Loss = 0.54249\n",
            "Epoch = 161 RMSE Training Loss = 0.54231\n",
            "Epoch = 162 RMSE Training Loss = 0.54214\n",
            "Epoch = 163 RMSE Training Loss = 0.54196\n",
            "Epoch = 164 RMSE Training Loss = 0.54178\n",
            "Epoch = 165 RMSE Training Loss = 0.54160\n",
            "Epoch = 166 RMSE Training Loss = 0.54143\n",
            "Epoch = 167 RMSE Training Loss = 0.54125\n",
            "Epoch = 168 RMSE Training Loss = 0.54107\n",
            "Epoch = 169 RMSE Training Loss = 0.54089\n",
            "Epoch = 170 RMSE Training Loss = 0.54071\n",
            "Epoch = 171 RMSE Training Loss = 0.54053\n",
            "Epoch = 172 RMSE Training Loss = 0.54036\n",
            "Epoch = 173 RMSE Training Loss = 0.54018\n",
            "Epoch = 174 RMSE Training Loss = 0.54000\n",
            "Epoch = 175 RMSE Training Loss = 0.53982\n",
            "Epoch = 176 RMSE Training Loss = 0.53964\n",
            "Epoch = 177 RMSE Training Loss = 0.53946\n",
            "Epoch = 178 RMSE Training Loss = 0.53928\n",
            "Epoch = 179 RMSE Training Loss = 0.53909\n",
            "Epoch = 180 RMSE Training Loss = 0.53891\n",
            "Epoch = 181 RMSE Training Loss = 0.53873\n",
            "Epoch = 182 RMSE Training Loss = 0.53854\n",
            "Epoch = 183 RMSE Training Loss = 0.53836\n",
            "Epoch = 184 RMSE Training Loss = 0.53817\n",
            "Epoch = 185 RMSE Training Loss = 0.53798\n",
            "Epoch = 186 RMSE Training Loss = 0.53779\n",
            "Epoch = 187 RMSE Training Loss = 0.53759\n",
            "Epoch = 188 RMSE Training Loss = 0.53740\n",
            "Epoch = 189 RMSE Training Loss = 0.53720\n",
            "Epoch = 190 RMSE Training Loss = 0.53700\n",
            "Epoch = 191 RMSE Training Loss = 0.53680\n",
            "Epoch = 192 RMSE Training Loss = 0.53659\n",
            "Epoch = 193 RMSE Training Loss = 0.53638\n",
            "Epoch = 194 RMSE Training Loss = 0.53617\n",
            "Epoch = 195 RMSE Training Loss = 0.53596\n",
            "Epoch = 196 RMSE Training Loss = 0.53574\n",
            "Epoch = 197 RMSE Training Loss = 0.53552\n",
            "Epoch = 198 RMSE Training Loss = 0.53530\n",
            "Epoch = 199 RMSE Training Loss = 0.53507\n",
            "Epoch = 200 RMSE Training Loss = 0.53485\n",
            "Epoch = 201 RMSE Training Loss = 0.53462\n",
            "Epoch = 202 RMSE Training Loss = 0.53439\n",
            "Epoch = 203 RMSE Training Loss = 0.53416\n",
            "Epoch = 204 RMSE Training Loss = 0.53393\n",
            "Epoch = 205 RMSE Training Loss = 0.53370\n",
            "Epoch = 206 RMSE Training Loss = 0.53346\n",
            "Epoch = 207 RMSE Training Loss = 0.53323\n",
            "Epoch = 208 RMSE Training Loss = 0.53299\n",
            "Epoch = 209 RMSE Training Loss = 0.53276\n",
            "Epoch = 210 RMSE Training Loss = 0.53252\n",
            "Epoch = 211 RMSE Training Loss = 0.53228\n",
            "Epoch = 212 RMSE Training Loss = 0.53205\n",
            "Epoch = 213 RMSE Training Loss = 0.53181\n",
            "Epoch = 214 RMSE Training Loss = 0.53157\n",
            "Epoch = 215 RMSE Training Loss = 0.53133\n",
            "Epoch = 216 RMSE Training Loss = 0.53110\n",
            "Epoch = 217 RMSE Training Loss = 0.53086\n",
            "Epoch = 218 RMSE Training Loss = 0.53062\n",
            "Epoch = 219 RMSE Training Loss = 0.53038\n",
            "Epoch = 220 RMSE Training Loss = 0.53014\n",
            "Epoch = 221 RMSE Training Loss = 0.52991\n",
            "Epoch = 222 RMSE Training Loss = 0.52967\n",
            "Epoch = 223 RMSE Training Loss = 0.52943\n",
            "Epoch = 224 RMSE Training Loss = 0.52919\n",
            "Epoch = 225 RMSE Training Loss = 0.52895\n",
            "Epoch = 226 RMSE Training Loss = 0.52871\n",
            "Epoch = 227 RMSE Training Loss = 0.52847\n",
            "Epoch = 228 RMSE Training Loss = 0.52823\n",
            "Epoch = 229 RMSE Training Loss = 0.52799\n",
            "Epoch = 230 RMSE Training Loss = 0.52775\n",
            "Epoch = 231 RMSE Training Loss = 0.52751\n",
            "Epoch = 232 RMSE Training Loss = 0.52727\n",
            "Epoch = 233 RMSE Training Loss = 0.52702\n",
            "Epoch = 234 RMSE Training Loss = 0.52678\n",
            "Epoch = 235 RMSE Training Loss = 0.52654\n",
            "Epoch = 236 RMSE Training Loss = 0.52629\n",
            "Epoch = 237 RMSE Training Loss = 0.52605\n",
            "Epoch = 238 RMSE Training Loss = 0.52581\n",
            "Epoch = 239 RMSE Training Loss = 0.52556\n",
            "Epoch = 240 RMSE Training Loss = 0.52532\n",
            "Epoch = 241 RMSE Training Loss = 0.52507\n",
            "Epoch = 242 RMSE Training Loss = 0.52483\n",
            "Epoch = 243 RMSE Training Loss = 0.52459\n",
            "Epoch = 244 RMSE Training Loss = 0.52434\n",
            "Epoch = 245 RMSE Training Loss = 0.52410\n",
            "Epoch = 246 RMSE Training Loss = 0.52386\n",
            "Epoch = 247 RMSE Training Loss = 0.52362\n",
            "Epoch = 248 RMSE Training Loss = 0.52338\n",
            "Epoch = 249 RMSE Training Loss = 0.52314\n",
            "Epoch = 250 RMSE Training Loss = 0.52290\n",
            "Epoch = 251 RMSE Training Loss = 0.52267\n",
            "Epoch = 252 RMSE Training Loss = 0.52244\n",
            "Epoch = 253 RMSE Training Loss = 0.52221\n",
            "Epoch = 254 RMSE Training Loss = 0.52198\n",
            "Epoch = 255 RMSE Training Loss = 0.52175\n",
            "Epoch = 256 RMSE Training Loss = 0.52153\n",
            "Epoch = 257 RMSE Training Loss = 0.52131\n",
            "Epoch = 258 RMSE Training Loss = 0.52109\n",
            "Epoch = 259 RMSE Training Loss = 0.52088\n",
            "Epoch = 260 RMSE Training Loss = 0.52067\n",
            "Epoch = 261 RMSE Training Loss = 0.52046\n",
            "Epoch = 262 RMSE Training Loss = 0.52026\n",
            "Epoch = 263 RMSE Training Loss = 0.52006\n",
            "Epoch = 264 RMSE Training Loss = 0.51987\n",
            "Epoch = 265 RMSE Training Loss = 0.51968\n",
            "Epoch = 266 RMSE Training Loss = 0.51949\n",
            "Epoch = 267 RMSE Training Loss = 0.51931\n",
            "Epoch = 268 RMSE Training Loss = 0.51913\n",
            "Epoch = 269 RMSE Training Loss = 0.51895\n",
            "Epoch = 270 RMSE Training Loss = 0.51878\n",
            "Epoch = 271 RMSE Training Loss = 0.51862\n",
            "Epoch = 272 RMSE Training Loss = 0.51846\n",
            "Epoch = 273 RMSE Training Loss = 0.51830\n",
            "Epoch = 274 RMSE Training Loss = 0.51815\n",
            "Epoch = 275 RMSE Training Loss = 0.51800\n",
            "Epoch = 276 RMSE Training Loss = 0.51785\n",
            "Epoch = 277 RMSE Training Loss = 0.51771\n",
            "Epoch = 278 RMSE Training Loss = 0.51757\n",
            "Epoch = 279 RMSE Training Loss = 0.51744\n",
            "Epoch = 280 RMSE Training Loss = 0.51730\n",
            "Epoch = 281 RMSE Training Loss = 0.51718\n",
            "Epoch = 282 RMSE Training Loss = 0.51705\n",
            "Epoch = 283 RMSE Training Loss = 0.51693\n",
            "Epoch = 284 RMSE Training Loss = 0.51681\n",
            "Epoch = 285 RMSE Training Loss = 0.51669\n",
            "Epoch = 286 RMSE Training Loss = 0.51658\n",
            "Epoch = 287 RMSE Training Loss = 0.51647\n",
            "Epoch = 288 RMSE Training Loss = 0.51636\n",
            "Epoch = 289 RMSE Training Loss = 0.51626\n",
            "Epoch = 290 RMSE Training Loss = 0.51615\n",
            "Epoch = 291 RMSE Training Loss = 0.51605\n",
            "Epoch = 292 RMSE Training Loss = 0.51595\n",
            "Epoch = 293 RMSE Training Loss = 0.51586\n",
            "Epoch = 294 RMSE Training Loss = 0.51576\n",
            "Epoch = 295 RMSE Training Loss = 0.51567\n",
            "Epoch = 296 RMSE Training Loss = 0.51558\n",
            "Epoch = 297 RMSE Training Loss = 0.51549\n",
            "Epoch = 298 RMSE Training Loss = 0.51540\n",
            "Epoch = 299 RMSE Training Loss = 0.51531\n",
            "Epoch = 300 RMSE Training Loss = 0.51523\n",
            "Epoch = 301 RMSE Training Loss = 0.51515\n",
            "Epoch = 302 RMSE Training Loss = 0.51507\n",
            "Epoch = 303 RMSE Training Loss = 0.51499\n",
            "Epoch = 304 RMSE Training Loss = 0.51491\n",
            "Epoch = 305 RMSE Training Loss = 0.51483\n",
            "Epoch = 306 RMSE Training Loss = 0.51476\n",
            "Epoch = 307 RMSE Training Loss = 0.51469\n",
            "Epoch = 308 RMSE Training Loss = 0.51461\n",
            "Epoch = 309 RMSE Training Loss = 0.51454\n",
            "Epoch = 310 RMSE Training Loss = 0.51447\n",
            "Epoch = 311 RMSE Training Loss = 0.51441\n",
            "Epoch = 312 RMSE Training Loss = 0.51434\n",
            "Epoch = 313 RMSE Training Loss = 0.51428\n",
            "Epoch = 314 RMSE Training Loss = 0.51421\n",
            "Epoch = 315 RMSE Training Loss = 0.51415\n",
            "Epoch = 316 RMSE Training Loss = 0.51409\n",
            "Epoch = 317 RMSE Training Loss = 0.51403\n",
            "Epoch = 318 RMSE Training Loss = 0.51397\n",
            "Epoch = 319 RMSE Training Loss = 0.51391\n",
            "Epoch = 320 RMSE Training Loss = 0.51385\n",
            "Epoch = 321 RMSE Training Loss = 0.51380\n",
            "Epoch = 322 RMSE Training Loss = 0.51374\n",
            "Epoch = 323 RMSE Training Loss = 0.51369\n",
            "Epoch = 324 RMSE Training Loss = 0.51364\n",
            "Epoch = 325 RMSE Training Loss = 0.51359\n",
            "Epoch = 326 RMSE Training Loss = 0.51353\n",
            "Epoch = 327 RMSE Training Loss = 0.51349\n",
            "Epoch = 328 RMSE Training Loss = 0.51344\n",
            "Epoch = 329 RMSE Training Loss = 0.51339\n",
            "Epoch = 330 RMSE Training Loss = 0.51334\n",
            "Epoch = 331 RMSE Training Loss = 0.51330\n",
            "Epoch = 332 RMSE Training Loss = 0.51325\n",
            "Epoch = 333 RMSE Training Loss = 0.51321\n",
            "Epoch = 334 RMSE Training Loss = 0.51317\n",
            "Epoch = 335 RMSE Training Loss = 0.51312\n",
            "Epoch = 336 RMSE Training Loss = 0.51308\n",
            "Epoch = 337 RMSE Training Loss = 0.51304\n",
            "Epoch = 338 RMSE Training Loss = 0.51300\n",
            "Epoch = 339 RMSE Training Loss = 0.51296\n",
            "Epoch = 340 RMSE Training Loss = 0.51293\n",
            "Epoch = 341 RMSE Training Loss = 0.51289\n",
            "Epoch = 342 RMSE Training Loss = 0.51285\n",
            "Epoch = 343 RMSE Training Loss = 0.51282\n",
            "Epoch = 344 RMSE Training Loss = 0.51278\n",
            "Epoch = 345 RMSE Training Loss = 0.51275\n",
            "Epoch = 346 RMSE Training Loss = 0.51271\n",
            "Epoch = 347 RMSE Training Loss = 0.51268\n",
            "Epoch = 348 RMSE Training Loss = 0.51265\n",
            "Epoch = 349 RMSE Training Loss = 0.51262\n",
            "Epoch = 350 RMSE Training Loss = 0.51259\n",
            "Epoch = 351 RMSE Training Loss = 0.51256\n",
            "Epoch = 352 RMSE Training Loss = 0.51253\n",
            "Epoch = 353 RMSE Training Loss = 0.51250\n",
            "Epoch = 354 RMSE Training Loss = 0.51247\n",
            "Epoch = 355 RMSE Training Loss = 0.51244\n",
            "Epoch = 356 RMSE Training Loss = 0.51242\n",
            "Epoch = 357 RMSE Training Loss = 0.51239\n",
            "Epoch = 358 RMSE Training Loss = 0.51236\n",
            "Epoch = 359 RMSE Training Loss = 0.51234\n",
            "Epoch = 360 RMSE Training Loss = 0.51231\n",
            "Epoch = 361 RMSE Training Loss = 0.51229\n",
            "Epoch = 362 RMSE Training Loss = 0.51227\n",
            "Epoch = 363 RMSE Training Loss = 0.51224\n",
            "Epoch = 364 RMSE Training Loss = 0.51222\n",
            "Epoch = 365 RMSE Training Loss = 0.51220\n",
            "Epoch = 366 RMSE Training Loss = 0.51218\n",
            "Epoch = 367 RMSE Training Loss = 0.51216\n",
            "Epoch = 368 RMSE Training Loss = 0.51214\n",
            "Epoch = 369 RMSE Training Loss = 0.51212\n",
            "Epoch = 370 RMSE Training Loss = 0.51210\n",
            "Epoch = 371 RMSE Training Loss = 0.51209\n",
            "Epoch = 372 RMSE Training Loss = 0.51207\n",
            "Epoch = 373 RMSE Training Loss = 0.51205\n",
            "Epoch = 374 RMSE Training Loss = 0.51203\n",
            "Epoch = 375 RMSE Training Loss = 0.51202\n",
            "Epoch = 376 RMSE Training Loss = 0.51200\n",
            "Epoch = 377 RMSE Training Loss = 0.51199\n",
            "Epoch = 378 RMSE Training Loss = 0.51198\n",
            "Epoch = 379 RMSE Training Loss = 0.51196\n",
            "Epoch = 380 RMSE Training Loss = 0.51195\n",
            "Epoch = 381 RMSE Training Loss = 0.51194\n",
            "Epoch = 382 RMSE Training Loss = 0.51193\n",
            "Epoch = 383 RMSE Training Loss = 0.51192\n",
            "Epoch = 384 RMSE Training Loss = 0.51191\n",
            "Epoch = 385 RMSE Training Loss = 0.51190\n",
            "Epoch = 386 RMSE Training Loss = 0.51189\n",
            "Epoch = 387 RMSE Training Loss = 0.51188\n",
            "Epoch = 388 RMSE Training Loss = 0.51188\n",
            "Epoch = 389 RMSE Training Loss = 0.51187\n",
            "Epoch = 390 RMSE Training Loss = 0.51187\n",
            "Epoch = 391 RMSE Training Loss = 0.51186\n",
            "Epoch = 392 RMSE Training Loss = 0.51186\n",
            "Epoch = 393 RMSE Training Loss = 0.51185\n",
            "Epoch = 394 RMSE Training Loss = 0.51185\n",
            "Epoch = 395 RMSE Training Loss = 0.51185\n",
            "Epoch = 396 RMSE Training Loss = 0.51185\n",
            "Epoch = 397 RMSE Training Loss = 0.51185\n",
            "Epoch = 398 RMSE Training Loss = 0.51185\n",
            "Epoch = 399 RMSE Training Loss = 0.51186\n",
            "Epoch = 400 RMSE Training Loss = 0.51186\n",
            "Epoch = 401 RMSE Training Loss = 0.51187\n",
            "Epoch = 402 RMSE Training Loss = 0.51187\n",
            "Epoch = 403 RMSE Training Loss = 0.51188\n",
            "Epoch = 404 RMSE Training Loss = 0.51189\n",
            "Epoch = 405 RMSE Training Loss = 0.51190\n",
            "Epoch = 406 RMSE Training Loss = 0.51191\n",
            "Epoch = 407 RMSE Training Loss = 0.51192\n",
            "Epoch = 408 RMSE Training Loss = 0.51193\n",
            "Epoch = 409 RMSE Training Loss = 0.51195\n",
            "Epoch = 410 RMSE Training Loss = 0.51197\n",
            "Epoch = 411 RMSE Training Loss = 0.51198\n",
            "Epoch = 412 RMSE Training Loss = 0.51200\n",
            "Epoch = 413 RMSE Training Loss = 0.51202\n",
            "Epoch = 414 RMSE Training Loss = 0.51205\n",
            "Epoch = 415 RMSE Training Loss = 0.51207\n",
            "Epoch = 416 RMSE Training Loss = 0.51209\n",
            "Epoch = 417 RMSE Training Loss = 0.51212\n",
            "Epoch = 418 RMSE Training Loss = 0.51215\n",
            "Epoch = 419 RMSE Training Loss = 0.51218\n",
            "Epoch = 420 RMSE Training Loss = 0.51221\n",
            "Epoch = 421 RMSE Training Loss = 0.51225\n",
            "Epoch = 422 RMSE Training Loss = 0.51228\n",
            "Epoch = 423 RMSE Training Loss = 0.51232\n",
            "Epoch = 424 RMSE Training Loss = 0.51236\n",
            "Epoch = 425 RMSE Training Loss = 0.51240\n",
            "Epoch = 426 RMSE Training Loss = 0.51244\n",
            "Epoch = 427 RMSE Training Loss = 0.51248\n",
            "Epoch = 428 RMSE Training Loss = 0.51253\n",
            "Epoch = 429 RMSE Training Loss = 0.51257\n",
            "Epoch = 430 RMSE Training Loss = 0.51262\n",
            "Epoch = 431 RMSE Training Loss = 0.51267\n",
            "Epoch = 432 RMSE Training Loss = 0.51272\n",
            "Epoch = 433 RMSE Training Loss = 0.51278\n",
            "Epoch = 434 RMSE Training Loss = 0.51283\n",
            "Epoch = 435 RMSE Training Loss = 0.51288\n",
            "Epoch = 436 RMSE Training Loss = 0.51294\n",
            "Epoch = 437 RMSE Training Loss = 0.51300\n",
            "Epoch = 438 RMSE Training Loss = 0.51306\n",
            "Epoch = 439 RMSE Training Loss = 0.51312\n",
            "Epoch = 440 RMSE Training Loss = 0.51318\n",
            "Epoch = 441 RMSE Training Loss = 0.51324\n",
            "Epoch = 442 RMSE Training Loss = 0.51330\n",
            "Epoch = 443 RMSE Training Loss = 0.51336\n",
            "Epoch = 444 RMSE Training Loss = 0.51342\n",
            "Epoch = 445 RMSE Training Loss = 0.51349\n",
            "Epoch = 446 RMSE Training Loss = 0.51355\n",
            "Epoch = 447 RMSE Training Loss = 0.51362\n",
            "Epoch = 448 RMSE Training Loss = 0.51368\n",
            "Epoch = 449 RMSE Training Loss = 0.51375\n",
            "Epoch = 450 RMSE Training Loss = 0.51381\n",
            "Epoch = 451 RMSE Training Loss = 0.51388\n",
            "Epoch = 452 RMSE Training Loss = 0.51395\n",
            "Epoch = 453 RMSE Training Loss = 0.51401\n",
            "Epoch = 454 RMSE Training Loss = 0.51408\n",
            "Epoch = 455 RMSE Training Loss = 0.51414\n",
            "Epoch = 456 RMSE Training Loss = 0.51421\n",
            "Epoch = 457 RMSE Training Loss = 0.51428\n",
            "Epoch = 458 RMSE Training Loss = 0.51434\n",
            "Epoch = 459 RMSE Training Loss = 0.51441\n",
            "Epoch = 460 RMSE Training Loss = 0.51447\n",
            "Epoch = 461 RMSE Training Loss = 0.51454\n",
            "Epoch = 462 RMSE Training Loss = 0.51460\n",
            "Epoch = 463 RMSE Training Loss = 0.51467\n",
            "Epoch = 464 RMSE Training Loss = 0.51473\n",
            "Epoch = 465 RMSE Training Loss = 0.51479\n",
            "Epoch = 466 RMSE Training Loss = 0.51486\n",
            "Epoch = 467 RMSE Training Loss = 0.51492\n",
            "Epoch = 468 RMSE Training Loss = 0.51498\n",
            "Epoch = 469 RMSE Training Loss = 0.51505\n",
            "Epoch = 470 RMSE Training Loss = 0.51511\n",
            "Epoch = 471 RMSE Training Loss = 0.51517\n",
            "Epoch = 472 RMSE Training Loss = 0.51523\n",
            "Epoch = 473 RMSE Training Loss = 0.51529\n",
            "Epoch = 474 RMSE Training Loss = 0.51535\n",
            "Epoch = 475 RMSE Training Loss = 0.51541\n",
            "Epoch = 476 RMSE Training Loss = 0.51547\n",
            "Epoch = 477 RMSE Training Loss = 0.51552\n",
            "Epoch = 478 RMSE Training Loss = 0.51558\n",
            "Epoch = 479 RMSE Training Loss = 0.51564\n",
            "Epoch = 480 RMSE Training Loss = 0.51569\n",
            "Epoch = 481 RMSE Training Loss = 0.51575\n",
            "Epoch = 482 RMSE Training Loss = 0.51581\n",
            "Epoch = 483 RMSE Training Loss = 0.51586\n",
            "Epoch = 484 RMSE Training Loss = 0.51591\n",
            "Epoch = 485 RMSE Training Loss = 0.51597\n",
            "Epoch = 486 RMSE Training Loss = 0.51602\n",
            "Epoch = 487 RMSE Training Loss = 0.51607\n",
            "Epoch = 488 RMSE Training Loss = 0.51613\n",
            "Epoch = 489 RMSE Training Loss = 0.51618\n",
            "Epoch = 490 RMSE Training Loss = 0.51623\n",
            "Epoch = 491 RMSE Training Loss = 0.51628\n",
            "Epoch = 492 RMSE Training Loss = 0.51633\n",
            "Epoch = 493 RMSE Training Loss = 0.51638\n",
            "Epoch = 494 RMSE Training Loss = 0.51643\n",
            "Epoch = 495 RMSE Training Loss = 0.51647\n",
            "Epoch = 496 RMSE Training Loss = 0.51652\n",
            "Epoch = 497 RMSE Training Loss = 0.51657\n",
            "Epoch = 498 RMSE Training Loss = 0.51661\n",
            "Epoch = 499 RMSE Training Loss = 0.51666\n",
            "Epoch = 500 RMSE Training Loss = 0.51671\n",
            "Epoch = 501 RMSE Training Loss = 0.51675\n",
            "Epoch = 502 RMSE Training Loss = 0.51679\n",
            "Epoch = 503 RMSE Training Loss = 0.51684\n",
            "Epoch = 504 RMSE Training Loss = 0.51688\n",
            "Epoch = 505 RMSE Training Loss = 0.51692\n",
            "Epoch = 506 RMSE Training Loss = 0.51697\n",
            "Epoch = 507 RMSE Training Loss = 0.51701\n",
            "Epoch = 508 RMSE Training Loss = 0.51705\n",
            "Epoch = 509 RMSE Training Loss = 0.51709\n",
            "Epoch = 510 RMSE Training Loss = 0.51713\n",
            "Epoch = 511 RMSE Training Loss = 0.51717\n",
            "Epoch = 512 RMSE Training Loss = 0.51721\n",
            "Epoch = 513 RMSE Training Loss = 0.51725\n",
            "Epoch = 514 RMSE Training Loss = 0.51729\n",
            "Epoch = 515 RMSE Training Loss = 0.51733\n",
            "Epoch = 516 RMSE Training Loss = 0.51736\n",
            "Epoch = 517 RMSE Training Loss = 0.51740\n",
            "Epoch = 518 RMSE Training Loss = 0.51744\n",
            "Epoch = 519 RMSE Training Loss = 0.51747\n",
            "Epoch = 520 RMSE Training Loss = 0.51751\n",
            "Epoch = 521 RMSE Training Loss = 0.51755\n",
            "Epoch = 522 RMSE Training Loss = 0.51758\n",
            "Epoch = 523 RMSE Training Loss = 0.51762\n",
            "Epoch = 524 RMSE Training Loss = 0.51765\n",
            "Epoch = 525 RMSE Training Loss = 0.51768\n",
            "Epoch = 526 RMSE Training Loss = 0.51772\n",
            "Epoch = 527 RMSE Training Loss = 0.51775\n",
            "Epoch = 528 RMSE Training Loss = 0.51778\n",
            "Epoch = 529 RMSE Training Loss = 0.51782\n",
            "Epoch = 530 RMSE Training Loss = 0.51785\n",
            "Epoch = 531 RMSE Training Loss = 0.51788\n",
            "Epoch = 532 RMSE Training Loss = 0.51791\n",
            "Epoch = 533 RMSE Training Loss = 0.51794\n",
            "Epoch = 534 RMSE Training Loss = 0.51797\n",
            "Epoch = 535 RMSE Training Loss = 0.51800\n",
            "Epoch = 536 RMSE Training Loss = 0.51803\n",
            "Epoch = 537 RMSE Training Loss = 0.51806\n",
            "Epoch = 538 RMSE Training Loss = 0.51809\n",
            "Epoch = 539 RMSE Training Loss = 0.51812\n",
            "Epoch = 540 RMSE Training Loss = 0.51815\n",
            "Epoch = 541 RMSE Training Loss = 0.51818\n",
            "Epoch = 542 RMSE Training Loss = 0.51821\n",
            "Epoch = 543 RMSE Training Loss = 0.51823\n",
            "Epoch = 544 RMSE Training Loss = 0.51826\n",
            "Epoch = 545 RMSE Training Loss = 0.51829\n",
            "Epoch = 546 RMSE Training Loss = 0.51832\n",
            "Epoch = 547 RMSE Training Loss = 0.51834\n",
            "Epoch = 548 RMSE Training Loss = 0.51837\n",
            "Epoch = 549 RMSE Training Loss = 0.51840\n",
            "Epoch = 550 RMSE Training Loss = 0.51842\n",
            "Epoch = 551 RMSE Training Loss = 0.51845\n",
            "Epoch = 552 RMSE Training Loss = 0.51847\n",
            "Epoch = 553 RMSE Training Loss = 0.51850\n",
            "Epoch = 554 RMSE Training Loss = 0.51852\n",
            "Epoch = 555 RMSE Training Loss = 0.51855\n",
            "Epoch = 556 RMSE Training Loss = 0.51857\n",
            "Epoch = 557 RMSE Training Loss = 0.51859\n",
            "Epoch = 558 RMSE Training Loss = 0.51862\n",
            "Epoch = 559 RMSE Training Loss = 0.51864\n",
            "Epoch = 560 RMSE Training Loss = 0.51867\n",
            "Epoch = 561 RMSE Training Loss = 0.51869\n",
            "Epoch = 562 RMSE Training Loss = 0.51871\n",
            "Epoch = 563 RMSE Training Loss = 0.51873\n",
            "Epoch = 564 RMSE Training Loss = 0.51876\n",
            "Epoch = 565 RMSE Training Loss = 0.51878\n",
            "Epoch = 566 RMSE Training Loss = 0.51880\n",
            "Epoch = 567 RMSE Training Loss = 0.51882\n",
            "Epoch = 568 RMSE Training Loss = 0.51884\n",
            "Epoch = 569 RMSE Training Loss = 0.51886\n",
            "Epoch = 570 RMSE Training Loss = 0.51889\n",
            "Epoch = 571 RMSE Training Loss = 0.51891\n",
            "Epoch = 572 RMSE Training Loss = 0.51893\n",
            "Epoch = 573 RMSE Training Loss = 0.51895\n",
            "Epoch = 574 RMSE Training Loss = 0.51897\n",
            "Epoch = 575 RMSE Training Loss = 0.51899\n",
            "Epoch = 576 RMSE Training Loss = 0.51901\n",
            "Epoch = 577 RMSE Training Loss = 0.51903\n",
            "Epoch = 578 RMSE Training Loss = 0.51905\n",
            "Epoch = 579 RMSE Training Loss = 0.51906\n",
            "Epoch = 580 RMSE Training Loss = 0.51908\n",
            "Epoch = 581 RMSE Training Loss = 0.51910\n",
            "Epoch = 582 RMSE Training Loss = 0.51912\n",
            "Epoch = 583 RMSE Training Loss = 0.51914\n",
            "Epoch = 584 RMSE Training Loss = 0.51916\n",
            "Epoch = 585 RMSE Training Loss = 0.51918\n",
            "Epoch = 586 RMSE Training Loss = 0.51919\n",
            "Epoch = 587 RMSE Training Loss = 0.51921\n",
            "Epoch = 588 RMSE Training Loss = 0.51923\n",
            "Epoch = 589 RMSE Training Loss = 0.51925\n",
            "Epoch = 590 RMSE Training Loss = 0.51926\n",
            "Epoch = 591 RMSE Training Loss = 0.51928\n",
            "Epoch = 592 RMSE Training Loss = 0.51930\n",
            "Epoch = 593 RMSE Training Loss = 0.51931\n",
            "Epoch = 594 RMSE Training Loss = 0.51933\n",
            "Epoch = 595 RMSE Training Loss = 0.51934\n",
            "Epoch = 596 RMSE Training Loss = 0.51936\n",
            "Epoch = 597 RMSE Training Loss = 0.51938\n",
            "Epoch = 598 RMSE Training Loss = 0.51939\n",
            "Epoch = 599 RMSE Training Loss = 0.51941\n",
            "Epoch = 600 RMSE Training Loss = 0.51942\n",
            "Epoch = 601 RMSE Training Loss = 0.51944\n",
            "Epoch = 602 RMSE Training Loss = 0.51945\n",
            "Epoch = 603 RMSE Training Loss = 0.51947\n",
            "Epoch = 604 RMSE Training Loss = 0.51948\n",
            "Epoch = 605 RMSE Training Loss = 0.51950\n",
            "Epoch = 606 RMSE Training Loss = 0.51951\n",
            "Epoch = 607 RMSE Training Loss = 0.51953\n",
            "Epoch = 608 RMSE Training Loss = 0.51954\n",
            "Epoch = 609 RMSE Training Loss = 0.51955\n",
            "Epoch = 610 RMSE Training Loss = 0.51957\n",
            "Epoch = 611 RMSE Training Loss = 0.51958\n",
            "Epoch = 612 RMSE Training Loss = 0.51959\n",
            "Epoch = 613 RMSE Training Loss = 0.51961\n",
            "Epoch = 614 RMSE Training Loss = 0.51962\n",
            "Epoch = 615 RMSE Training Loss = 0.51963\n",
            "Epoch = 616 RMSE Training Loss = 0.51965\n",
            "Epoch = 617 RMSE Training Loss = 0.51966\n",
            "Epoch = 618 RMSE Training Loss = 0.51967\n",
            "Epoch = 619 RMSE Training Loss = 0.51968\n",
            "Epoch = 620 RMSE Training Loss = 0.51970\n",
            "Epoch = 621 RMSE Training Loss = 0.51971\n",
            "Epoch = 622 RMSE Training Loss = 0.51972\n",
            "Epoch = 623 RMSE Training Loss = 0.51973\n",
            "Epoch = 624 RMSE Training Loss = 0.51974\n",
            "Epoch = 625 RMSE Training Loss = 0.51975\n",
            "Epoch = 626 RMSE Training Loss = 0.51976\n",
            "Epoch = 627 RMSE Training Loss = 0.51978\n",
            "Epoch = 628 RMSE Training Loss = 0.51979\n",
            "Epoch = 629 RMSE Training Loss = 0.51980\n",
            "Epoch = 630 RMSE Training Loss = 0.51981\n",
            "Epoch = 631 RMSE Training Loss = 0.51982\n",
            "Epoch = 632 RMSE Training Loss = 0.51983\n",
            "Epoch = 633 RMSE Training Loss = 0.51984\n",
            "Epoch = 634 RMSE Training Loss = 0.51985\n",
            "Epoch = 635 RMSE Training Loss = 0.51986\n",
            "Epoch = 636 RMSE Training Loss = 0.51987\n",
            "Epoch = 637 RMSE Training Loss = 0.51988\n",
            "Epoch = 638 RMSE Training Loss = 0.51989\n",
            "Epoch = 639 RMSE Training Loss = 0.51989\n",
            "Epoch = 640 RMSE Training Loss = 0.51990\n",
            "Epoch = 641 RMSE Training Loss = 0.51991\n",
            "Epoch = 642 RMSE Training Loss = 0.51992\n",
            "Epoch = 643 RMSE Training Loss = 0.51993\n",
            "Epoch = 644 RMSE Training Loss = 0.51994\n",
            "Epoch = 645 RMSE Training Loss = 0.51995\n",
            "Epoch = 646 RMSE Training Loss = 0.51995\n",
            "Epoch = 647 RMSE Training Loss = 0.51996\n",
            "Epoch = 648 RMSE Training Loss = 0.51997\n",
            "Epoch = 649 RMSE Training Loss = 0.51998\n",
            "Epoch = 650 RMSE Training Loss = 0.51998\n",
            "Epoch = 651 RMSE Training Loss = 0.51999\n",
            "Epoch = 652 RMSE Training Loss = 0.52000\n",
            "Epoch = 653 RMSE Training Loss = 0.52000\n",
            "Epoch = 654 RMSE Training Loss = 0.52001\n",
            "Epoch = 655 RMSE Training Loss = 0.52002\n",
            "Epoch = 656 RMSE Training Loss = 0.52002\n",
            "Epoch = 657 RMSE Training Loss = 0.52003\n",
            "Epoch = 658 RMSE Training Loss = 0.52004\n",
            "Epoch = 659 RMSE Training Loss = 0.52004\n",
            "Epoch = 660 RMSE Training Loss = 0.52005\n",
            "Epoch = 661 RMSE Training Loss = 0.52005\n",
            "Epoch = 662 RMSE Training Loss = 0.52006\n",
            "Epoch = 663 RMSE Training Loss = 0.52006\n",
            "Epoch = 664 RMSE Training Loss = 0.52007\n",
            "Epoch = 665 RMSE Training Loss = 0.52007\n",
            "Epoch = 666 RMSE Training Loss = 0.52008\n",
            "Epoch = 667 RMSE Training Loss = 0.52008\n",
            "Epoch = 668 RMSE Training Loss = 0.52009\n",
            "Epoch = 669 RMSE Training Loss = 0.52009\n",
            "Epoch = 670 RMSE Training Loss = 0.52009\n",
            "Epoch = 671 RMSE Training Loss = 0.52010\n",
            "Epoch = 672 RMSE Training Loss = 0.52010\n",
            "Epoch = 673 RMSE Training Loss = 0.52010\n",
            "Epoch = 674 RMSE Training Loss = 0.52011\n",
            "Epoch = 675 RMSE Training Loss = 0.52011\n",
            "Epoch = 676 RMSE Training Loss = 0.52011\n",
            "Epoch = 677 RMSE Training Loss = 0.52011\n",
            "Epoch = 678 RMSE Training Loss = 0.52012\n",
            "Epoch = 679 RMSE Training Loss = 0.52012\n",
            "Epoch = 680 RMSE Training Loss = 0.52012\n",
            "Epoch = 681 RMSE Training Loss = 0.52012\n",
            "Epoch = 682 RMSE Training Loss = 0.52012\n",
            "Epoch = 683 RMSE Training Loss = 0.52013\n",
            "Epoch = 684 RMSE Training Loss = 0.52013\n",
            "Epoch = 685 RMSE Training Loss = 0.52013\n",
            "Epoch = 686 RMSE Training Loss = 0.52013\n",
            "Epoch = 687 RMSE Training Loss = 0.52013\n",
            "Epoch = 688 RMSE Training Loss = 0.52013\n",
            "Epoch = 689 RMSE Training Loss = 0.52013\n",
            "Epoch = 690 RMSE Training Loss = 0.52013\n",
            "Epoch = 691 RMSE Training Loss = 0.52013\n",
            "Epoch = 692 RMSE Training Loss = 0.52013\n",
            "Epoch = 693 RMSE Training Loss = 0.52013\n",
            "Epoch = 694 RMSE Training Loss = 0.52013\n",
            "Epoch = 695 RMSE Training Loss = 0.52013\n",
            "Epoch = 696 RMSE Training Loss = 0.52013\n",
            "Epoch = 697 RMSE Training Loss = 0.52012\n",
            "Epoch = 698 RMSE Training Loss = 0.52012\n",
            "Epoch = 699 RMSE Training Loss = 0.52012\n",
            "Epoch = 700 RMSE Training Loss = 0.52012\n",
            "Epoch = 701 RMSE Training Loss = 0.52012\n",
            "Epoch = 702 RMSE Training Loss = 0.52011\n",
            "Epoch = 703 RMSE Training Loss = 0.52011\n",
            "Epoch = 704 RMSE Training Loss = 0.52011\n",
            "Epoch = 705 RMSE Training Loss = 0.52011\n",
            "Epoch = 706 RMSE Training Loss = 0.52010\n",
            "Epoch = 707 RMSE Training Loss = 0.52010\n",
            "Epoch = 708 RMSE Training Loss = 0.52010\n",
            "Epoch = 709 RMSE Training Loss = 0.52009\n",
            "Epoch = 710 RMSE Training Loss = 0.52009\n",
            "Epoch = 711 RMSE Training Loss = 0.52008\n",
            "Epoch = 712 RMSE Training Loss = 0.52008\n",
            "Epoch = 713 RMSE Training Loss = 0.52007\n",
            "Epoch = 714 RMSE Training Loss = 0.52007\n",
            "Epoch = 715 RMSE Training Loss = 0.52006\n",
            "Epoch = 716 RMSE Training Loss = 0.52006\n",
            "Epoch = 717 RMSE Training Loss = 0.52005\n",
            "Epoch = 718 RMSE Training Loss = 0.52005\n",
            "Epoch = 719 RMSE Training Loss = 0.52004\n",
            "Epoch = 720 RMSE Training Loss = 0.52004\n",
            "Epoch = 721 RMSE Training Loss = 0.52003\n",
            "Epoch = 722 RMSE Training Loss = 0.52003\n",
            "Epoch = 723 RMSE Training Loss = 0.52002\n",
            "Epoch = 724 RMSE Training Loss = 0.52001\n",
            "Epoch = 725 RMSE Training Loss = 0.52001\n",
            "Epoch = 726 RMSE Training Loss = 0.52000\n",
            "Epoch = 727 RMSE Training Loss = 0.51999\n",
            "Epoch = 728 RMSE Training Loss = 0.51998\n",
            "Epoch = 729 RMSE Training Loss = 0.51998\n",
            "Epoch = 730 RMSE Training Loss = 0.51997\n",
            "Epoch = 731 RMSE Training Loss = 0.51996\n",
            "Epoch = 732 RMSE Training Loss = 0.51996\n",
            "Epoch = 733 RMSE Training Loss = 0.51995\n",
            "Epoch = 734 RMSE Training Loss = 0.51994\n",
            "Epoch = 735 RMSE Training Loss = 0.51993\n",
            "Epoch = 736 RMSE Training Loss = 0.51992\n",
            "Epoch = 737 RMSE Training Loss = 0.51992\n",
            "Epoch = 738 RMSE Training Loss = 0.51991\n",
            "Epoch = 739 RMSE Training Loss = 0.51990\n",
            "Epoch = 740 RMSE Training Loss = 0.51989\n",
            "Epoch = 741 RMSE Training Loss = 0.51988\n",
            "Epoch = 742 RMSE Training Loss = 0.51987\n",
            "Epoch = 743 RMSE Training Loss = 0.51987\n",
            "Epoch = 744 RMSE Training Loss = 0.51986\n",
            "Epoch = 745 RMSE Training Loss = 0.51985\n",
            "Epoch = 746 RMSE Training Loss = 0.51984\n",
            "Epoch = 747 RMSE Training Loss = 0.51983\n",
            "Epoch = 748 RMSE Training Loss = 0.51982\n",
            "Epoch = 749 RMSE Training Loss = 0.51981\n",
            "Epoch = 750 RMSE Training Loss = 0.51980\n",
            "Epoch = 751 RMSE Training Loss = 0.51980\n",
            "Epoch = 752 RMSE Training Loss = 0.51979\n",
            "Epoch = 753 RMSE Training Loss = 0.51978\n",
            "Epoch = 754 RMSE Training Loss = 0.51977\n",
            "Epoch = 755 RMSE Training Loss = 0.51976\n",
            "Epoch = 756 RMSE Training Loss = 0.51975\n",
            "Epoch = 757 RMSE Training Loss = 0.51975\n",
            "Epoch = 758 RMSE Training Loss = 0.51974\n",
            "Epoch = 759 RMSE Training Loss = 0.51973\n",
            "Epoch = 760 RMSE Training Loss = 0.51972\n",
            "Epoch = 761 RMSE Training Loss = 0.51971\n",
            "Epoch = 762 RMSE Training Loss = 0.51970\n",
            "Epoch = 763 RMSE Training Loss = 0.51970\n",
            "Epoch = 764 RMSE Training Loss = 0.51969\n",
            "Epoch = 765 RMSE Training Loss = 0.51968\n",
            "Epoch = 766 RMSE Training Loss = 0.51967\n",
            "Epoch = 767 RMSE Training Loss = 0.51967\n",
            "Epoch = 768 RMSE Training Loss = 0.51966\n",
            "Epoch = 769 RMSE Training Loss = 0.51965\n",
            "Epoch = 770 RMSE Training Loss = 0.51965\n",
            "Epoch = 771 RMSE Training Loss = 0.51964\n",
            "Epoch = 772 RMSE Training Loss = 0.51964\n",
            "Epoch = 773 RMSE Training Loss = 0.51963\n",
            "Epoch = 774 RMSE Training Loss = 0.51962\n",
            "Epoch = 775 RMSE Training Loss = 0.51962\n",
            "Epoch = 776 RMSE Training Loss = 0.51961\n",
            "Epoch = 777 RMSE Training Loss = 0.51961\n",
            "Epoch = 778 RMSE Training Loss = 0.51961\n",
            "Epoch = 779 RMSE Training Loss = 0.51960\n",
            "Epoch = 780 RMSE Training Loss = 0.51960\n",
            "Epoch = 781 RMSE Training Loss = 0.51959\n",
            "Epoch = 782 RMSE Training Loss = 0.51959\n",
            "Epoch = 783 RMSE Training Loss = 0.51959\n",
            "Epoch = 784 RMSE Training Loss = 0.51959\n",
            "Epoch = 785 RMSE Training Loss = 0.51959\n",
            "Epoch = 786 RMSE Training Loss = 0.51958\n",
            "Epoch = 787 RMSE Training Loss = 0.51958\n",
            "Epoch = 788 RMSE Training Loss = 0.51958\n",
            "Epoch = 789 RMSE Training Loss = 0.51958\n",
            "Epoch = 790 RMSE Training Loss = 0.51958\n",
            "Epoch = 791 RMSE Training Loss = 0.51958\n",
            "Epoch = 792 RMSE Training Loss = 0.51958\n",
            "Epoch = 793 RMSE Training Loss = 0.51959\n",
            "Epoch = 794 RMSE Training Loss = 0.51959\n",
            "Epoch = 795 RMSE Training Loss = 0.51959\n",
            "Epoch = 796 RMSE Training Loss = 0.51959\n",
            "Epoch = 797 RMSE Training Loss = 0.51960\n",
            "Epoch = 798 RMSE Training Loss = 0.51960\n",
            "Epoch = 799 RMSE Training Loss = 0.51961\n",
            "Epoch = 800 RMSE Training Loss = 0.51961\n",
            "Epoch = 801 RMSE Training Loss = 0.51962\n",
            "Epoch = 802 RMSE Training Loss = 0.51962\n",
            "Epoch = 803 RMSE Training Loss = 0.51963\n",
            "Epoch = 804 RMSE Training Loss = 0.51964\n",
            "Epoch = 805 RMSE Training Loss = 0.51964\n",
            "Epoch = 806 RMSE Training Loss = 0.51965\n",
            "Epoch = 807 RMSE Training Loss = 0.51966\n",
            "Epoch = 808 RMSE Training Loss = 0.51967\n",
            "Epoch = 809 RMSE Training Loss = 0.51968\n",
            "Epoch = 810 RMSE Training Loss = 0.51969\n",
            "Epoch = 811 RMSE Training Loss = 0.51970\n",
            "Epoch = 812 RMSE Training Loss = 0.51972\n",
            "Epoch = 813 RMSE Training Loss = 0.51973\n",
            "Epoch = 814 RMSE Training Loss = 0.51974\n",
            "Epoch = 815 RMSE Training Loss = 0.51976\n",
            "Epoch = 816 RMSE Training Loss = 0.51977\n",
            "Epoch = 817 RMSE Training Loss = 0.51979\n",
            "Epoch = 818 RMSE Training Loss = 0.51980\n",
            "Epoch = 819 RMSE Training Loss = 0.51982\n",
            "Epoch = 820 RMSE Training Loss = 0.51984\n",
            "Epoch = 821 RMSE Training Loss = 0.51985\n",
            "Epoch = 822 RMSE Training Loss = 0.51987\n",
            "Epoch = 823 RMSE Training Loss = 0.51989\n",
            "Epoch = 824 RMSE Training Loss = 0.51991\n",
            "Epoch = 825 RMSE Training Loss = 0.51993\n",
            "Epoch = 826 RMSE Training Loss = 0.51995\n",
            "Epoch = 827 RMSE Training Loss = 0.51998\n",
            "Epoch = 828 RMSE Training Loss = 0.52000\n",
            "Epoch = 829 RMSE Training Loss = 0.52002\n",
            "Epoch = 830 RMSE Training Loss = 0.52005\n",
            "Epoch = 831 RMSE Training Loss = 0.52007\n",
            "Epoch = 832 RMSE Training Loss = 0.52010\n",
            "Epoch = 833 RMSE Training Loss = 0.52013\n",
            "Epoch = 834 RMSE Training Loss = 0.52015\n",
            "Epoch = 835 RMSE Training Loss = 0.52018\n",
            "Epoch = 836 RMSE Training Loss = 0.52021\n",
            "Epoch = 837 RMSE Training Loss = 0.52024\n",
            "Epoch = 838 RMSE Training Loss = 0.52027\n",
            "Epoch = 839 RMSE Training Loss = 0.52031\n",
            "Epoch = 840 RMSE Training Loss = 0.52034\n",
            "Epoch = 841 RMSE Training Loss = 0.52037\n",
            "Epoch = 842 RMSE Training Loss = 0.52041\n",
            "Epoch = 843 RMSE Training Loss = 0.52044\n",
            "Epoch = 844 RMSE Training Loss = 0.52048\n",
            "Epoch = 845 RMSE Training Loss = 0.52052\n",
            "Epoch = 846 RMSE Training Loss = 0.52056\n",
            "Epoch = 847 RMSE Training Loss = 0.52060\n",
            "Epoch = 848 RMSE Training Loss = 0.52064\n",
            "Epoch = 849 RMSE Training Loss = 0.52068\n",
            "Epoch = 850 RMSE Training Loss = 0.52072\n",
            "Epoch = 851 RMSE Training Loss = 0.52077\n",
            "Epoch = 852 RMSE Training Loss = 0.52081\n",
            "Epoch = 853 RMSE Training Loss = 0.52086\n",
            "Epoch = 854 RMSE Training Loss = 0.52090\n",
            "Epoch = 855 RMSE Training Loss = 0.52095\n",
            "Epoch = 856 RMSE Training Loss = 0.52100\n",
            "Epoch = 857 RMSE Training Loss = 0.52105\n",
            "Epoch = 858 RMSE Training Loss = 0.52110\n",
            "Epoch = 859 RMSE Training Loss = 0.52116\n",
            "Epoch = 860 RMSE Training Loss = 0.52121\n",
            "Epoch = 861 RMSE Training Loss = 0.52127\n",
            "Epoch = 862 RMSE Training Loss = 0.52132\n",
            "Epoch = 863 RMSE Training Loss = 0.52138\n",
            "Epoch = 864 RMSE Training Loss = 0.52144\n",
            "Epoch = 865 RMSE Training Loss = 0.52150\n",
            "Epoch = 866 RMSE Training Loss = 0.52156\n",
            "Epoch = 867 RMSE Training Loss = 0.52162\n",
            "Epoch = 868 RMSE Training Loss = 0.52169\n",
            "Epoch = 869 RMSE Training Loss = 0.52175\n",
            "Epoch = 870 RMSE Training Loss = 0.52182\n",
            "Epoch = 871 RMSE Training Loss = 0.52189\n",
            "Epoch = 872 RMSE Training Loss = 0.52196\n",
            "Epoch = 873 RMSE Training Loss = 0.52203\n",
            "Epoch = 874 RMSE Training Loss = 0.52210\n",
            "Epoch = 875 RMSE Training Loss = 0.52218\n",
            "Epoch = 876 RMSE Training Loss = 0.52225\n",
            "Epoch = 877 RMSE Training Loss = 0.52233\n",
            "Epoch = 878 RMSE Training Loss = 0.52241\n",
            "Epoch = 879 RMSE Training Loss = 0.52249\n",
            "Epoch = 880 RMSE Training Loss = 0.52257\n",
            "Epoch = 881 RMSE Training Loss = 0.52265\n",
            "Epoch = 882 RMSE Training Loss = 0.52273\n",
            "Epoch = 883 RMSE Training Loss = 0.52282\n",
            "Epoch = 884 RMSE Training Loss = 0.52291\n",
            "Epoch = 885 RMSE Training Loss = 0.52299\n",
            "Epoch = 886 RMSE Training Loss = 0.52308\n",
            "Epoch = 887 RMSE Training Loss = 0.52317\n",
            "Epoch = 888 RMSE Training Loss = 0.52326\n",
            "Epoch = 889 RMSE Training Loss = 0.52336\n",
            "Epoch = 890 RMSE Training Loss = 0.52345\n",
            "Epoch = 891 RMSE Training Loss = 0.52355\n",
            "Epoch = 892 RMSE Training Loss = 0.52365\n",
            "Epoch = 893 RMSE Training Loss = 0.52374\n",
            "Epoch = 894 RMSE Training Loss = 0.52384\n",
            "Epoch = 895 RMSE Training Loss = 0.52395\n",
            "Epoch = 896 RMSE Training Loss = 0.52405\n",
            "Epoch = 897 RMSE Training Loss = 0.52415\n",
            "Epoch = 898 RMSE Training Loss = 0.52426\n",
            "Epoch = 899 RMSE Training Loss = 0.52436\n",
            "Epoch = 900 RMSE Training Loss = 0.52447\n",
            "Epoch = 901 RMSE Training Loss = 0.52458\n",
            "Epoch = 902 RMSE Training Loss = 0.52469\n",
            "Epoch = 903 RMSE Training Loss = 0.52480\n",
            "Epoch = 904 RMSE Training Loss = 0.52491\n",
            "Epoch = 905 RMSE Training Loss = 0.52502\n",
            "Epoch = 906 RMSE Training Loss = 0.52513\n",
            "Epoch = 907 RMSE Training Loss = 0.52525\n",
            "Epoch = 908 RMSE Training Loss = 0.52536\n",
            "Epoch = 909 RMSE Training Loss = 0.52548\n",
            "Epoch = 910 RMSE Training Loss = 0.52560\n",
            "Epoch = 911 RMSE Training Loss = 0.52571\n",
            "Epoch = 912 RMSE Training Loss = 0.52583\n",
            "Epoch = 913 RMSE Training Loss = 0.52595\n",
            "Epoch = 914 RMSE Training Loss = 0.52607\n",
            "Epoch = 915 RMSE Training Loss = 0.52619\n",
            "Epoch = 916 RMSE Training Loss = 0.52632\n",
            "Epoch = 917 RMSE Training Loss = 0.52644\n",
            "Epoch = 918 RMSE Training Loss = 0.52656\n",
            "Epoch = 919 RMSE Training Loss = 0.52668\n",
            "Epoch = 920 RMSE Training Loss = 0.52681\n",
            "Epoch = 921 RMSE Training Loss = 0.52693\n",
            "Epoch = 922 RMSE Training Loss = 0.52706\n",
            "Epoch = 923 RMSE Training Loss = 0.52718\n",
            "Epoch = 924 RMSE Training Loss = 0.52731\n",
            "Epoch = 925 RMSE Training Loss = 0.52744\n",
            "Epoch = 926 RMSE Training Loss = 0.52756\n",
            "Epoch = 927 RMSE Training Loss = 0.52769\n",
            "Epoch = 928 RMSE Training Loss = 0.52782\n",
            "Epoch = 929 RMSE Training Loss = 0.52794\n",
            "Epoch = 930 RMSE Training Loss = 0.52807\n",
            "Epoch = 931 RMSE Training Loss = 0.52820\n",
            "Epoch = 932 RMSE Training Loss = 0.52833\n",
            "Epoch = 933 RMSE Training Loss = 0.52846\n",
            "Epoch = 934 RMSE Training Loss = 0.52858\n",
            "Epoch = 935 RMSE Training Loss = 0.52871\n",
            "Epoch = 936 RMSE Training Loss = 0.52884\n",
            "Epoch = 937 RMSE Training Loss = 0.52897\n",
            "Epoch = 938 RMSE Training Loss = 0.52910\n",
            "Epoch = 939 RMSE Training Loss = 0.52922\n",
            "Epoch = 940 RMSE Training Loss = 0.52935\n",
            "Epoch = 941 RMSE Training Loss = 0.52948\n",
            "Epoch = 942 RMSE Training Loss = 0.52961\n",
            "Epoch = 943 RMSE Training Loss = 0.52974\n",
            "Epoch = 944 RMSE Training Loss = 0.52986\n",
            "Epoch = 945 RMSE Training Loss = 0.52999\n",
            "Epoch = 946 RMSE Training Loss = 0.53012\n",
            "Epoch = 947 RMSE Training Loss = 0.53024\n",
            "Epoch = 948 RMSE Training Loss = 0.53037\n",
            "Epoch = 949 RMSE Training Loss = 0.53050\n",
            "Epoch = 950 RMSE Training Loss = 0.53062\n",
            "Epoch = 951 RMSE Training Loss = 0.53075\n",
            "Epoch = 952 RMSE Training Loss = 0.53087\n",
            "Epoch = 953 RMSE Training Loss = 0.53100\n",
            "Epoch = 954 RMSE Training Loss = 0.53112\n",
            "Epoch = 955 RMSE Training Loss = 0.53124\n",
            "Epoch = 956 RMSE Training Loss = 0.53137\n",
            "Epoch = 957 RMSE Training Loss = 0.53149\n",
            "Epoch = 958 RMSE Training Loss = 0.53161\n",
            "Epoch = 959 RMSE Training Loss = 0.53173\n",
            "Epoch = 960 RMSE Training Loss = 0.53185\n",
            "Epoch = 961 RMSE Training Loss = 0.53197\n",
            "Epoch = 962 RMSE Training Loss = 0.53209\n",
            "Epoch = 963 RMSE Training Loss = 0.53221\n",
            "Epoch = 964 RMSE Training Loss = 0.53233\n",
            "Epoch = 965 RMSE Training Loss = 0.53245\n",
            "Epoch = 966 RMSE Training Loss = 0.53256\n",
            "Epoch = 967 RMSE Training Loss = 0.53268\n",
            "Epoch = 968 RMSE Training Loss = 0.53280\n",
            "Epoch = 969 RMSE Training Loss = 0.53291\n",
            "Epoch = 970 RMSE Training Loss = 0.53303\n",
            "Epoch = 971 RMSE Training Loss = 0.53314\n",
            "Epoch = 972 RMSE Training Loss = 0.53325\n",
            "Epoch = 973 RMSE Training Loss = 0.53336\n",
            "Epoch = 974 RMSE Training Loss = 0.53348\n",
            "Epoch = 975 RMSE Training Loss = 0.53359\n",
            "Epoch = 976 RMSE Training Loss = 0.53370\n",
            "Epoch = 977 RMSE Training Loss = 0.53381\n",
            "Epoch = 978 RMSE Training Loss = 0.53391\n",
            "Epoch = 979 RMSE Training Loss = 0.53402\n",
            "Epoch = 980 RMSE Training Loss = 0.53413\n",
            "Epoch = 981 RMSE Training Loss = 0.53424\n",
            "Epoch = 982 RMSE Training Loss = 0.53434\n",
            "Epoch = 983 RMSE Training Loss = 0.53445\n",
            "Epoch = 984 RMSE Training Loss = 0.53455\n",
            "Epoch = 985 RMSE Training Loss = 0.53465\n",
            "Epoch = 986 RMSE Training Loss = 0.53475\n",
            "Epoch = 987 RMSE Training Loss = 0.53486\n",
            "Epoch = 988 RMSE Training Loss = 0.53496\n",
            "Epoch = 989 RMSE Training Loss = 0.53506\n",
            "Epoch = 990 RMSE Training Loss = 0.53516\n",
            "Epoch = 991 RMSE Training Loss = 0.53525\n",
            "Epoch = 992 RMSE Training Loss = 0.53535\n",
            "Epoch = 993 RMSE Training Loss = 0.53545\n",
            "Epoch = 994 RMSE Training Loss = 0.53554\n",
            "Epoch = 995 RMSE Training Loss = 0.53564\n",
            "Epoch = 996 RMSE Training Loss = 0.53573\n",
            "Epoch = 997 RMSE Training Loss = 0.53583\n",
            "Epoch = 998 RMSE Training Loss = 0.53592\n",
            "Epoch = 999 RMSE Training Loss = 0.53601\n",
            "Epoch = 1000 RMSE Training Loss = 0.53610\n",
            "Epoch = 1001 RMSE Training Loss = 0.53619\n",
            "Epoch = 1002 RMSE Training Loss = 0.53628\n",
            "Epoch = 1003 RMSE Training Loss = 0.53637\n",
            "Epoch = 1004 RMSE Training Loss = 0.53646\n",
            "Epoch = 1005 RMSE Training Loss = 0.53654\n",
            "Epoch = 1006 RMSE Training Loss = 0.53663\n",
            "Epoch = 1007 RMSE Training Loss = 0.53671\n",
            "Epoch = 1008 RMSE Training Loss = 0.53680\n",
            "Epoch = 1009 RMSE Training Loss = 0.53688\n",
            "Epoch = 1010 RMSE Training Loss = 0.53697\n",
            "Epoch = 1011 RMSE Training Loss = 0.53705\n",
            "Epoch = 1012 RMSE Training Loss = 0.53713\n",
            "Epoch = 1013 RMSE Training Loss = 0.53721\n",
            "Epoch = 1014 RMSE Training Loss = 0.53729\n",
            "Epoch = 1015 RMSE Training Loss = 0.53737\n",
            "Epoch = 1016 RMSE Training Loss = 0.53745\n",
            "Epoch = 1017 RMSE Training Loss = 0.53752\n",
            "Epoch = 1018 RMSE Training Loss = 0.53760\n",
            "Epoch = 1019 RMSE Training Loss = 0.53768\n",
            "Epoch = 1020 RMSE Training Loss = 0.53775\n",
            "Epoch = 1021 RMSE Training Loss = 0.53783\n",
            "Epoch = 1022 RMSE Training Loss = 0.53790\n",
            "Epoch = 1023 RMSE Training Loss = 0.53797\n",
            "Epoch = 1024 RMSE Training Loss = 0.53804\n",
            "Epoch = 1025 RMSE Training Loss = 0.53812\n",
            "Epoch = 1026 RMSE Training Loss = 0.53819\n",
            "Epoch = 1027 RMSE Training Loss = 0.53826\n",
            "Epoch = 1028 RMSE Training Loss = 0.53833\n",
            "Epoch = 1029 RMSE Training Loss = 0.53840\n",
            "Epoch = 1030 RMSE Training Loss = 0.53846\n",
            "Epoch = 1031 RMSE Training Loss = 0.53853\n",
            "Epoch = 1032 RMSE Training Loss = 0.53860\n",
            "Epoch = 1033 RMSE Training Loss = 0.53866\n",
            "Epoch = 1034 RMSE Training Loss = 0.53873\n",
            "Epoch = 1035 RMSE Training Loss = 0.53879\n",
            "Epoch = 1036 RMSE Training Loss = 0.53886\n",
            "Epoch = 1037 RMSE Training Loss = 0.53892\n",
            "Epoch = 1038 RMSE Training Loss = 0.53898\n",
            "Epoch = 1039 RMSE Training Loss = 0.53904\n",
            "Epoch = 1040 RMSE Training Loss = 0.53911\n",
            "Epoch = 1041 RMSE Training Loss = 0.53917\n",
            "Epoch = 1042 RMSE Training Loss = 0.53923\n",
            "Epoch = 1043 RMSE Training Loss = 0.53929\n",
            "Epoch = 1044 RMSE Training Loss = 0.53934\n",
            "Epoch = 1045 RMSE Training Loss = 0.53940\n",
            "Epoch = 1046 RMSE Training Loss = 0.53946\n",
            "Epoch = 1047 RMSE Training Loss = 0.53952\n",
            "Epoch = 1048 RMSE Training Loss = 0.53957\n",
            "Epoch = 1049 RMSE Training Loss = 0.53963\n",
            "Epoch = 1050 RMSE Training Loss = 0.53968\n",
            "Epoch = 1051 RMSE Training Loss = 0.53974\n",
            "Epoch = 1052 RMSE Training Loss = 0.53979\n",
            "Epoch = 1053 RMSE Training Loss = 0.53985\n",
            "Epoch = 1054 RMSE Training Loss = 0.53990\n",
            "Epoch = 1055 RMSE Training Loss = 0.53995\n",
            "Epoch = 1056 RMSE Training Loss = 0.54000\n",
            "Epoch = 1057 RMSE Training Loss = 0.54005\n",
            "Epoch = 1058 RMSE Training Loss = 0.54010\n",
            "Epoch = 1059 RMSE Training Loss = 0.54015\n",
            "Epoch = 1060 RMSE Training Loss = 0.54020\n",
            "Epoch = 1061 RMSE Training Loss = 0.54025\n",
            "Epoch = 1062 RMSE Training Loss = 0.54030\n",
            "Epoch = 1063 RMSE Training Loss = 0.54035\n",
            "Epoch = 1064 RMSE Training Loss = 0.54040\n",
            "Epoch = 1065 RMSE Training Loss = 0.54044\n",
            "Epoch = 1066 RMSE Training Loss = 0.54049\n",
            "Epoch = 1067 RMSE Training Loss = 0.54054\n",
            "Epoch = 1068 RMSE Training Loss = 0.54058\n",
            "Epoch = 1069 RMSE Training Loss = 0.54063\n",
            "Epoch = 1070 RMSE Training Loss = 0.54067\n",
            "Epoch = 1071 RMSE Training Loss = 0.54072\n",
            "Epoch = 1072 RMSE Training Loss = 0.54076\n",
            "Epoch = 1073 RMSE Training Loss = 0.54080\n",
            "Epoch = 1074 RMSE Training Loss = 0.54084\n",
            "Epoch = 1075 RMSE Training Loss = 0.54089\n",
            "Epoch = 1076 RMSE Training Loss = 0.54093\n",
            "Epoch = 1077 RMSE Training Loss = 0.54097\n",
            "Epoch = 1078 RMSE Training Loss = 0.54101\n",
            "Epoch = 1079 RMSE Training Loss = 0.54105\n",
            "Epoch = 1080 RMSE Training Loss = 0.54109\n",
            "Epoch = 1081 RMSE Training Loss = 0.54113\n",
            "Epoch = 1082 RMSE Training Loss = 0.54117\n",
            "Epoch = 1083 RMSE Training Loss = 0.54121\n",
            "Epoch = 1084 RMSE Training Loss = 0.54125\n",
            "Epoch = 1085 RMSE Training Loss = 0.54128\n",
            "Epoch = 1086 RMSE Training Loss = 0.54132\n",
            "Epoch = 1087 RMSE Training Loss = 0.54136\n",
            "Epoch = 1088 RMSE Training Loss = 0.54139\n",
            "Epoch = 1089 RMSE Training Loss = 0.54143\n",
            "Epoch = 1090 RMSE Training Loss = 0.54147\n",
            "Epoch = 1091 RMSE Training Loss = 0.54150\n",
            "Epoch = 1092 RMSE Training Loss = 0.54154\n",
            "Epoch = 1093 RMSE Training Loss = 0.54157\n",
            "Epoch = 1094 RMSE Training Loss = 0.54161\n",
            "Epoch = 1095 RMSE Training Loss = 0.54164\n",
            "Epoch = 1096 RMSE Training Loss = 0.54167\n",
            "Epoch = 1097 RMSE Training Loss = 0.54171\n",
            "Epoch = 1098 RMSE Training Loss = 0.54174\n",
            "Epoch = 1099 RMSE Training Loss = 0.54177\n",
            "Epoch = 1100 RMSE Training Loss = 0.54180\n",
            "Epoch = 1101 RMSE Training Loss = 0.54184\n",
            "Epoch = 1102 RMSE Training Loss = 0.54187\n",
            "Epoch = 1103 RMSE Training Loss = 0.54190\n",
            "Epoch = 1104 RMSE Training Loss = 0.54193\n",
            "Epoch = 1105 RMSE Training Loss = 0.54196\n",
            "Epoch = 1106 RMSE Training Loss = 0.54199\n",
            "Epoch = 1107 RMSE Training Loss = 0.54202\n",
            "Epoch = 1108 RMSE Training Loss = 0.54205\n",
            "Epoch = 1109 RMSE Training Loss = 0.54208\n",
            "Epoch = 1110 RMSE Training Loss = 0.54211\n",
            "Epoch = 1111 RMSE Training Loss = 0.54214\n",
            "Epoch = 1112 RMSE Training Loss = 0.54216\n",
            "Epoch = 1113 RMSE Training Loss = 0.54219\n",
            "Epoch = 1114 RMSE Training Loss = 0.54222\n",
            "Epoch = 1115 RMSE Training Loss = 0.54225\n",
            "Epoch = 1116 RMSE Training Loss = 0.54227\n",
            "Epoch = 1117 RMSE Training Loss = 0.54230\n",
            "Epoch = 1118 RMSE Training Loss = 0.54233\n",
            "Epoch = 1119 RMSE Training Loss = 0.54235\n",
            "Epoch = 1120 RMSE Training Loss = 0.54238\n",
            "Epoch = 1121 RMSE Training Loss = 0.54241\n",
            "Epoch = 1122 RMSE Training Loss = 0.54243\n",
            "Epoch = 1123 RMSE Training Loss = 0.54246\n",
            "Epoch = 1124 RMSE Training Loss = 0.54248\n",
            "Epoch = 1125 RMSE Training Loss = 0.54251\n",
            "Epoch = 1126 RMSE Training Loss = 0.54253\n",
            "Epoch = 1127 RMSE Training Loss = 0.54255\n",
            "Epoch = 1128 RMSE Training Loss = 0.54258\n",
            "Epoch = 1129 RMSE Training Loss = 0.54260\n",
            "Epoch = 1130 RMSE Training Loss = 0.54263\n",
            "Epoch = 1131 RMSE Training Loss = 0.54265\n",
            "Epoch = 1132 RMSE Training Loss = 0.54267\n",
            "Epoch = 1133 RMSE Training Loss = 0.54269\n",
            "Epoch = 1134 RMSE Training Loss = 0.54272\n",
            "Epoch = 1135 RMSE Training Loss = 0.54274\n",
            "Epoch = 1136 RMSE Training Loss = 0.54276\n",
            "Epoch = 1137 RMSE Training Loss = 0.54278\n",
            "Epoch = 1138 RMSE Training Loss = 0.54280\n",
            "Epoch = 1139 RMSE Training Loss = 0.54283\n",
            "Epoch = 1140 RMSE Training Loss = 0.54285\n",
            "Epoch = 1141 RMSE Training Loss = 0.54287\n",
            "Epoch = 1142 RMSE Training Loss = 0.54289\n",
            "Epoch = 1143 RMSE Training Loss = 0.54291\n",
            "Epoch = 1144 RMSE Training Loss = 0.54293\n",
            "Epoch = 1145 RMSE Training Loss = 0.54295\n",
            "Epoch = 1146 RMSE Training Loss = 0.54297\n",
            "Epoch = 1147 RMSE Training Loss = 0.54299\n",
            "Epoch = 1148 RMSE Training Loss = 0.54301\n",
            "Epoch = 1149 RMSE Training Loss = 0.54303\n",
            "Epoch = 1150 RMSE Training Loss = 0.54305\n",
            "Epoch = 1151 RMSE Training Loss = 0.54306\n",
            "Epoch = 1152 RMSE Training Loss = 0.54308\n",
            "Epoch = 1153 RMSE Training Loss = 0.54310\n",
            "Epoch = 1154 RMSE Training Loss = 0.54312\n",
            "Epoch = 1155 RMSE Training Loss = 0.54314\n",
            "Epoch = 1156 RMSE Training Loss = 0.54316\n",
            "Epoch = 1157 RMSE Training Loss = 0.54317\n",
            "Epoch = 1158 RMSE Training Loss = 0.54319\n",
            "Epoch = 1159 RMSE Training Loss = 0.54321\n",
            "Epoch = 1160 RMSE Training Loss = 0.54323\n",
            "Epoch = 1161 RMSE Training Loss = 0.54324\n",
            "Epoch = 1162 RMSE Training Loss = 0.54326\n",
            "Epoch = 1163 RMSE Training Loss = 0.54328\n",
            "Epoch = 1164 RMSE Training Loss = 0.54329\n",
            "Epoch = 1165 RMSE Training Loss = 0.54331\n",
            "Epoch = 1166 RMSE Training Loss = 0.54333\n",
            "Epoch = 1167 RMSE Training Loss = 0.54334\n",
            "Epoch = 1168 RMSE Training Loss = 0.54336\n",
            "Epoch = 1169 RMSE Training Loss = 0.54337\n",
            "Epoch = 1170 RMSE Training Loss = 0.54339\n",
            "Epoch = 1171 RMSE Training Loss = 0.54340\n",
            "Epoch = 1172 RMSE Training Loss = 0.54342\n",
            "Epoch = 1173 RMSE Training Loss = 0.54343\n",
            "Epoch = 1174 RMSE Training Loss = 0.54345\n",
            "Epoch = 1175 RMSE Training Loss = 0.54346\n",
            "Epoch = 1176 RMSE Training Loss = 0.54348\n",
            "Epoch = 1177 RMSE Training Loss = 0.54349\n",
            "Epoch = 1178 RMSE Training Loss = 0.54351\n",
            "Epoch = 1179 RMSE Training Loss = 0.54352\n",
            "Epoch = 1180 RMSE Training Loss = 0.54354\n",
            "Epoch = 1181 RMSE Training Loss = 0.54355\n",
            "Epoch = 1182 RMSE Training Loss = 0.54356\n",
            "Epoch = 1183 RMSE Training Loss = 0.54358\n",
            "Epoch = 1184 RMSE Training Loss = 0.54359\n",
            "Epoch = 1185 RMSE Training Loss = 0.54360\n",
            "Epoch = 1186 RMSE Training Loss = 0.54362\n",
            "Epoch = 1187 RMSE Training Loss = 0.54363\n",
            "Epoch = 1188 RMSE Training Loss = 0.54364\n",
            "Epoch = 1189 RMSE Training Loss = 0.54366\n",
            "Epoch = 1190 RMSE Training Loss = 0.54367\n",
            "Epoch = 1191 RMSE Training Loss = 0.54368\n",
            "Epoch = 1192 RMSE Training Loss = 0.54369\n",
            "Epoch = 1193 RMSE Training Loss = 0.54371\n",
            "Epoch = 1194 RMSE Training Loss = 0.54372\n",
            "Epoch = 1195 RMSE Training Loss = 0.54373\n",
            "Epoch = 1196 RMSE Training Loss = 0.54374\n",
            "Epoch = 1197 RMSE Training Loss = 0.54376\n",
            "Epoch = 1198 RMSE Training Loss = 0.54377\n",
            "Epoch = 1199 RMSE Training Loss = 0.54378\n",
            "Epoch = 1200 RMSE Training Loss = 0.54379\n",
            "Epoch = 1201 RMSE Training Loss = 0.54380\n",
            "Epoch = 1202 RMSE Training Loss = 0.54381\n",
            "Epoch = 1203 RMSE Training Loss = 0.54383\n",
            "Epoch = 1204 RMSE Training Loss = 0.54384\n",
            "Epoch = 1205 RMSE Training Loss = 0.54385\n",
            "Epoch = 1206 RMSE Training Loss = 0.54386\n",
            "Epoch = 1207 RMSE Training Loss = 0.54387\n",
            "Epoch = 1208 RMSE Training Loss = 0.54388\n",
            "Epoch = 1209 RMSE Training Loss = 0.54389\n",
            "Epoch = 1210 RMSE Training Loss = 0.54390\n",
            "Epoch = 1211 RMSE Training Loss = 0.54391\n",
            "Epoch = 1212 RMSE Training Loss = 0.54392\n",
            "Epoch = 1213 RMSE Training Loss = 0.54393\n",
            "Epoch = 1214 RMSE Training Loss = 0.54394\n",
            "Epoch = 1215 RMSE Training Loss = 0.54395\n",
            "Epoch = 1216 RMSE Training Loss = 0.54396\n",
            "Epoch = 1217 RMSE Training Loss = 0.54397\n",
            "Epoch = 1218 RMSE Training Loss = 0.54398\n",
            "Epoch = 1219 RMSE Training Loss = 0.54399\n",
            "Epoch = 1220 RMSE Training Loss = 0.54400\n",
            "Epoch = 1221 RMSE Training Loss = 0.54401\n",
            "Epoch = 1222 RMSE Training Loss = 0.54402\n",
            "Epoch = 1223 RMSE Training Loss = 0.54403\n",
            "Epoch = 1224 RMSE Training Loss = 0.54404\n",
            "Epoch = 1225 RMSE Training Loss = 0.54405\n",
            "Epoch = 1226 RMSE Training Loss = 0.54406\n",
            "Epoch = 1227 RMSE Training Loss = 0.54407\n",
            "Epoch = 1228 RMSE Training Loss = 0.54408\n",
            "Epoch = 1229 RMSE Training Loss = 0.54408\n",
            "Epoch = 1230 RMSE Training Loss = 0.54409\n",
            "Epoch = 1231 RMSE Training Loss = 0.54410\n",
            "Epoch = 1232 RMSE Training Loss = 0.54411\n",
            "Epoch = 1233 RMSE Training Loss = 0.54412\n",
            "Epoch = 1234 RMSE Training Loss = 0.54413\n",
            "Epoch = 1235 RMSE Training Loss = 0.54414\n",
            "Epoch = 1236 RMSE Training Loss = 0.54414\n",
            "Epoch = 1237 RMSE Training Loss = 0.54415\n",
            "Epoch = 1238 RMSE Training Loss = 0.54416\n",
            "Epoch = 1239 RMSE Training Loss = 0.54417\n",
            "Epoch = 1240 RMSE Training Loss = 0.54418\n",
            "Epoch = 1241 RMSE Training Loss = 0.54419\n",
            "Epoch = 1242 RMSE Training Loss = 0.54419\n",
            "Epoch = 1243 RMSE Training Loss = 0.54420\n",
            "Epoch = 1244 RMSE Training Loss = 0.54421\n",
            "Epoch = 1245 RMSE Training Loss = 0.54422\n",
            "Epoch = 1246 RMSE Training Loss = 0.54422\n",
            "Epoch = 1247 RMSE Training Loss = 0.54423\n",
            "Epoch = 1248 RMSE Training Loss = 0.54424\n",
            "Epoch = 1249 RMSE Training Loss = 0.54425\n",
            "Epoch = 1250 RMSE Training Loss = 0.54425\n",
            "Epoch = 1251 RMSE Training Loss = 0.54426\n",
            "Epoch = 1252 RMSE Training Loss = 0.54427\n",
            "Epoch = 1253 RMSE Training Loss = 0.54428\n",
            "Epoch = 1254 RMSE Training Loss = 0.54428\n",
            "Epoch = 1255 RMSE Training Loss = 0.54429\n",
            "Epoch = 1256 RMSE Training Loss = 0.54430\n",
            "Epoch = 1257 RMSE Training Loss = 0.54430\n",
            "Epoch = 1258 RMSE Training Loss = 0.54431\n",
            "Epoch = 1259 RMSE Training Loss = 0.54432\n",
            "Epoch = 1260 RMSE Training Loss = 0.54432\n",
            "Epoch = 1261 RMSE Training Loss = 0.54433\n",
            "Epoch = 1262 RMSE Training Loss = 0.54434\n",
            "Epoch = 1263 RMSE Training Loss = 0.54434\n",
            "Epoch = 1264 RMSE Training Loss = 0.54435\n",
            "Epoch = 1265 RMSE Training Loss = 0.54436\n",
            "Epoch = 1266 RMSE Training Loss = 0.54436\n",
            "Epoch = 1267 RMSE Training Loss = 0.54437\n",
            "Epoch = 1268 RMSE Training Loss = 0.54437\n",
            "Epoch = 1269 RMSE Training Loss = 0.54438\n",
            "Epoch = 1270 RMSE Training Loss = 0.54439\n",
            "Epoch = 1271 RMSE Training Loss = 0.54439\n",
            "Epoch = 1272 RMSE Training Loss = 0.54440\n",
            "Epoch = 1273 RMSE Training Loss = 0.54441\n",
            "Epoch = 1274 RMSE Training Loss = 0.54441\n",
            "Epoch = 1275 RMSE Training Loss = 0.54442\n",
            "Epoch = 1276 RMSE Training Loss = 0.54442\n",
            "Epoch = 1277 RMSE Training Loss = 0.54443\n",
            "Epoch = 1278 RMSE Training Loss = 0.54443\n",
            "Epoch = 1279 RMSE Training Loss = 0.54444\n",
            "Epoch = 1280 RMSE Training Loss = 0.54445\n",
            "Epoch = 1281 RMSE Training Loss = 0.54445\n",
            "Epoch = 1282 RMSE Training Loss = 0.54446\n",
            "Epoch = 1283 RMSE Training Loss = 0.54446\n",
            "Epoch = 1284 RMSE Training Loss = 0.54447\n",
            "Epoch = 1285 RMSE Training Loss = 0.54447\n",
            "Epoch = 1286 RMSE Training Loss = 0.54448\n",
            "Epoch = 1287 RMSE Training Loss = 0.54448\n",
            "Epoch = 1288 RMSE Training Loss = 0.54449\n",
            "Epoch = 1289 RMSE Training Loss = 0.54449\n",
            "Epoch = 1290 RMSE Training Loss = 0.54450\n",
            "Epoch = 1291 RMSE Training Loss = 0.54451\n",
            "Epoch = 1292 RMSE Training Loss = 0.54451\n",
            "Epoch = 1293 RMSE Training Loss = 0.54452\n",
            "Epoch = 1294 RMSE Training Loss = 0.54452\n",
            "Epoch = 1295 RMSE Training Loss = 0.54453\n",
            "Epoch = 1296 RMSE Training Loss = 0.54453\n",
            "Epoch = 1297 RMSE Training Loss = 0.54453\n",
            "Epoch = 1298 RMSE Training Loss = 0.54454\n",
            "Epoch = 1299 RMSE Training Loss = 0.54454\n",
            "Epoch = 1300 RMSE Training Loss = 0.54455\n",
            "Epoch = 1301 RMSE Training Loss = 0.54455\n",
            "Epoch = 1302 RMSE Training Loss = 0.54456\n",
            "Epoch = 1303 RMSE Training Loss = 0.54456\n",
            "Epoch = 1304 RMSE Training Loss = 0.54457\n",
            "Epoch = 1305 RMSE Training Loss = 0.54457\n",
            "Epoch = 1306 RMSE Training Loss = 0.54458\n",
            "Epoch = 1307 RMSE Training Loss = 0.54458\n",
            "Epoch = 1308 RMSE Training Loss = 0.54459\n",
            "Epoch = 1309 RMSE Training Loss = 0.54459\n",
            "Epoch = 1310 RMSE Training Loss = 0.54459\n",
            "Epoch = 1311 RMSE Training Loss = 0.54460\n",
            "Epoch = 1312 RMSE Training Loss = 0.54460\n",
            "Epoch = 1313 RMSE Training Loss = 0.54461\n",
            "Epoch = 1314 RMSE Training Loss = 0.54461\n",
            "Epoch = 1315 RMSE Training Loss = 0.54462\n",
            "Epoch = 1316 RMSE Training Loss = 0.54462\n",
            "Epoch = 1317 RMSE Training Loss = 0.54462\n",
            "Epoch = 1318 RMSE Training Loss = 0.54463\n",
            "Epoch = 1319 RMSE Training Loss = 0.54463\n",
            "Epoch = 1320 RMSE Training Loss = 0.54464\n",
            "Epoch = 1321 RMSE Training Loss = 0.54464\n",
            "Epoch = 1322 RMSE Training Loss = 0.54464\n",
            "Epoch = 1323 RMSE Training Loss = 0.54465\n",
            "Epoch = 1324 RMSE Training Loss = 0.54465\n",
            "Epoch = 1325 RMSE Training Loss = 0.54466\n",
            "Epoch = 1326 RMSE Training Loss = 0.54466\n",
            "Epoch = 1327 RMSE Training Loss = 0.54466\n",
            "Epoch = 1328 RMSE Training Loss = 0.54467\n",
            "Epoch = 1329 RMSE Training Loss = 0.54467\n",
            "Epoch = 1330 RMSE Training Loss = 0.54468\n",
            "Epoch = 1331 RMSE Training Loss = 0.54468\n",
            "Epoch = 1332 RMSE Training Loss = 0.54468\n",
            "Epoch = 1333 RMSE Training Loss = 0.54469\n",
            "Epoch = 1334 RMSE Training Loss = 0.54469\n",
            "Epoch = 1335 RMSE Training Loss = 0.54469\n",
            "Epoch = 1336 RMSE Training Loss = 0.54470\n",
            "Epoch = 1337 RMSE Training Loss = 0.54470\n",
            "Epoch = 1338 RMSE Training Loss = 0.54470\n",
            "Epoch = 1339 RMSE Training Loss = 0.54471\n",
            "Epoch = 1340 RMSE Training Loss = 0.54471\n",
            "Epoch = 1341 RMSE Training Loss = 0.54471\n",
            "Epoch = 1342 RMSE Training Loss = 0.54472\n",
            "Epoch = 1343 RMSE Training Loss = 0.54472\n",
            "Epoch = 1344 RMSE Training Loss = 0.54472\n",
            "Epoch = 1345 RMSE Training Loss = 0.54473\n",
            "Epoch = 1346 RMSE Training Loss = 0.54473\n",
            "Epoch = 1347 RMSE Training Loss = 0.54473\n",
            "Epoch = 1348 RMSE Training Loss = 0.54474\n",
            "Epoch = 1349 RMSE Training Loss = 0.54474\n",
            "Epoch = 1350 RMSE Training Loss = 0.54474\n",
            "Epoch = 1351 RMSE Training Loss = 0.54475\n",
            "Epoch = 1352 RMSE Training Loss = 0.54475\n",
            "Epoch = 1353 RMSE Training Loss = 0.54475\n",
            "Epoch = 1354 RMSE Training Loss = 0.54476\n",
            "Epoch = 1355 RMSE Training Loss = 0.54476\n",
            "Epoch = 1356 RMSE Training Loss = 0.54476\n",
            "Epoch = 1357 RMSE Training Loss = 0.54476\n",
            "Epoch = 1358 RMSE Training Loss = 0.54477\n",
            "Epoch = 1359 RMSE Training Loss = 0.54477\n",
            "Epoch = 1360 RMSE Training Loss = 0.54477\n",
            "Epoch = 1361 RMSE Training Loss = 0.54478\n",
            "Epoch = 1362 RMSE Training Loss = 0.54478\n",
            "Epoch = 1363 RMSE Training Loss = 0.54478\n",
            "Epoch = 1364 RMSE Training Loss = 0.54479\n",
            "Epoch = 1365 RMSE Training Loss = 0.54479\n",
            "Epoch = 1366 RMSE Training Loss = 0.54479\n",
            "Epoch = 1367 RMSE Training Loss = 0.54479\n",
            "Epoch = 1368 RMSE Training Loss = 0.54480\n",
            "Epoch = 1369 RMSE Training Loss = 0.54480\n",
            "Epoch = 1370 RMSE Training Loss = 0.54480\n",
            "Epoch = 1371 RMSE Training Loss = 0.54480\n",
            "Epoch = 1372 RMSE Training Loss = 0.54481\n",
            "Epoch = 1373 RMSE Training Loss = 0.54481\n",
            "Epoch = 1374 RMSE Training Loss = 0.54481\n",
            "Epoch = 1375 RMSE Training Loss = 0.54481\n",
            "Epoch = 1376 RMSE Training Loss = 0.54482\n",
            "Epoch = 1377 RMSE Training Loss = 0.54482\n",
            "Epoch = 1378 RMSE Training Loss = 0.54482\n",
            "Epoch = 1379 RMSE Training Loss = 0.54482\n",
            "Epoch = 1380 RMSE Training Loss = 0.54483\n",
            "Epoch = 1381 RMSE Training Loss = 0.54483\n",
            "Epoch = 1382 RMSE Training Loss = 0.54483\n",
            "Epoch = 1383 RMSE Training Loss = 0.54483\n",
            "Epoch = 1384 RMSE Training Loss = 0.54484\n",
            "Epoch = 1385 RMSE Training Loss = 0.54484\n",
            "Epoch = 1386 RMSE Training Loss = 0.54484\n",
            "Epoch = 1387 RMSE Training Loss = 0.54484\n",
            "Epoch = 1388 RMSE Training Loss = 0.54485\n",
            "Epoch = 1389 RMSE Training Loss = 0.54485\n",
            "Epoch = 1390 RMSE Training Loss = 0.54485\n",
            "Epoch = 1391 RMSE Training Loss = 0.54485\n",
            "Epoch = 1392 RMSE Training Loss = 0.54486\n",
            "Epoch = 1393 RMSE Training Loss = 0.54486\n",
            "Epoch = 1394 RMSE Training Loss = 0.54486\n",
            "Epoch = 1395 RMSE Training Loss = 0.54486\n",
            "Epoch = 1396 RMSE Training Loss = 0.54486\n",
            "Epoch = 1397 RMSE Training Loss = 0.54487\n",
            "Epoch = 1398 RMSE Training Loss = 0.54487\n",
            "Epoch = 1399 RMSE Training Loss = 0.54487\n",
            "Epoch = 1400 RMSE Training Loss = 0.54487\n",
            "Epoch = 1401 RMSE Training Loss = 0.54487\n",
            "Epoch = 1402 RMSE Training Loss = 0.54488\n",
            "Epoch = 1403 RMSE Training Loss = 0.54488\n",
            "Epoch = 1404 RMSE Training Loss = 0.54488\n",
            "Epoch = 1405 RMSE Training Loss = 0.54488\n",
            "Epoch = 1406 RMSE Training Loss = 0.54489\n",
            "Epoch = 1407 RMSE Training Loss = 0.54489\n",
            "Epoch = 1408 RMSE Training Loss = 0.54489\n",
            "Epoch = 1409 RMSE Training Loss = 0.54489\n",
            "Epoch = 1410 RMSE Training Loss = 0.54489\n",
            "Epoch = 1411 RMSE Training Loss = 0.54490\n",
            "Epoch = 1412 RMSE Training Loss = 0.54490\n",
            "Epoch = 1413 RMSE Training Loss = 0.54490\n",
            "Epoch = 1414 RMSE Training Loss = 0.54490\n",
            "Epoch = 1415 RMSE Training Loss = 0.54490\n",
            "Epoch = 1416 RMSE Training Loss = 0.54490\n",
            "Epoch = 1417 RMSE Training Loss = 0.54491\n",
            "Epoch = 1418 RMSE Training Loss = 0.54491\n",
            "Epoch = 1419 RMSE Training Loss = 0.54491\n",
            "Epoch = 1420 RMSE Training Loss = 0.54491\n",
            "Epoch = 1421 RMSE Training Loss = 0.54491\n",
            "Epoch = 1422 RMSE Training Loss = 0.54492\n",
            "Epoch = 1423 RMSE Training Loss = 0.54492\n",
            "Epoch = 1424 RMSE Training Loss = 0.54492\n",
            "Epoch = 1425 RMSE Training Loss = 0.54492\n",
            "Epoch = 1426 RMSE Training Loss = 0.54492\n",
            "Epoch = 1427 RMSE Training Loss = 0.54492\n",
            "Epoch = 1428 RMSE Training Loss = 0.54493\n",
            "Epoch = 1429 RMSE Training Loss = 0.54493\n",
            "Epoch = 1430 RMSE Training Loss = 0.54493\n",
            "Epoch = 1431 RMSE Training Loss = 0.54493\n",
            "Epoch = 1432 RMSE Training Loss = 0.54493\n",
            "Epoch = 1433 RMSE Training Loss = 0.54493\n",
            "Epoch = 1434 RMSE Training Loss = 0.54494\n",
            "Epoch = 1435 RMSE Training Loss = 0.54494\n",
            "Epoch = 1436 RMSE Training Loss = 0.54494\n",
            "Epoch = 1437 RMSE Training Loss = 0.54494\n",
            "Epoch = 1438 RMSE Training Loss = 0.54494\n",
            "Epoch = 1439 RMSE Training Loss = 0.54494\n",
            "Epoch = 1440 RMSE Training Loss = 0.54495\n",
            "Epoch = 1441 RMSE Training Loss = 0.54495\n",
            "Epoch = 1442 RMSE Training Loss = 0.54495\n",
            "Epoch = 1443 RMSE Training Loss = 0.54495\n",
            "Epoch = 1444 RMSE Training Loss = 0.54495\n",
            "Epoch = 1445 RMSE Training Loss = 0.54495\n",
            "Epoch = 1446 RMSE Training Loss = 0.54495\n",
            "Epoch = 1447 RMSE Training Loss = 0.54496\n",
            "Epoch = 1448 RMSE Training Loss = 0.54496\n",
            "Epoch = 1449 RMSE Training Loss = 0.54496\n",
            "Epoch = 1450 RMSE Training Loss = 0.54496\n",
            "Epoch = 1451 RMSE Training Loss = 0.54496\n",
            "Epoch = 1452 RMSE Training Loss = 0.54496\n",
            "Epoch = 1453 RMSE Training Loss = 0.54496\n",
            "Epoch = 1454 RMSE Training Loss = 0.54497\n",
            "Epoch = 1455 RMSE Training Loss = 0.54497\n",
            "Epoch = 1456 RMSE Training Loss = 0.54497\n",
            "Epoch = 1457 RMSE Training Loss = 0.54497\n",
            "Epoch = 1458 RMSE Training Loss = 0.54497\n",
            "Epoch = 1459 RMSE Training Loss = 0.54497\n",
            "Epoch = 1460 RMSE Training Loss = 0.54497\n",
            "Epoch = 1461 RMSE Training Loss = 0.54497\n",
            "Epoch = 1462 RMSE Training Loss = 0.54498\n",
            "Epoch = 1463 RMSE Training Loss = 0.54498\n",
            "Epoch = 1464 RMSE Training Loss = 0.54498\n",
            "Epoch = 1465 RMSE Training Loss = 0.54498\n",
            "Epoch = 1466 RMSE Training Loss = 0.54498\n",
            "Epoch = 1467 RMSE Training Loss = 0.54498\n",
            "Epoch = 1468 RMSE Training Loss = 0.54498\n",
            "Epoch = 1469 RMSE Training Loss = 0.54499\n",
            "Epoch = 1470 RMSE Training Loss = 0.54499\n",
            "Epoch = 1471 RMSE Training Loss = 0.54499\n",
            "Epoch = 1472 RMSE Training Loss = 0.54499\n",
            "Epoch = 1473 RMSE Training Loss = 0.54499\n",
            "Epoch = 1474 RMSE Training Loss = 0.54499\n",
            "Epoch = 1475 RMSE Training Loss = 0.54499\n",
            "Epoch = 1476 RMSE Training Loss = 0.54499\n",
            "Epoch = 1477 RMSE Training Loss = 0.54499\n",
            "Epoch = 1478 RMSE Training Loss = 0.54500\n",
            "Epoch = 1479 RMSE Training Loss = 0.54500\n",
            "Epoch = 1480 RMSE Training Loss = 0.54500\n",
            "Epoch = 1481 RMSE Training Loss = 0.54500\n",
            "Epoch = 1482 RMSE Training Loss = 0.54500\n",
            "Epoch = 1483 RMSE Training Loss = 0.54500\n",
            "Epoch = 1484 RMSE Training Loss = 0.54500\n",
            "Epoch = 1485 RMSE Training Loss = 0.54500\n",
            "Epoch = 1486 RMSE Training Loss = 0.54500\n",
            "Epoch = 1487 RMSE Training Loss = 0.54501\n",
            "Epoch = 1488 RMSE Training Loss = 0.54501\n",
            "Epoch = 1489 RMSE Training Loss = 0.54501\n",
            "Epoch = 1490 RMSE Training Loss = 0.54501\n",
            "Epoch = 1491 RMSE Training Loss = 0.54501\n",
            "Epoch = 1492 RMSE Training Loss = 0.54501\n",
            "Epoch = 1493 RMSE Training Loss = 0.54501\n",
            "Epoch = 1494 RMSE Training Loss = 0.54501\n",
            "Epoch = 1495 RMSE Training Loss = 0.54501\n",
            "Epoch = 1496 RMSE Training Loss = 0.54501\n",
            "Epoch = 1497 RMSE Training Loss = 0.54502\n",
            "Epoch = 1498 RMSE Training Loss = 0.54502\n",
            "Epoch = 1499 RMSE Training Loss = 0.54502\n",
            "Epoch = 1500 RMSE Training Loss = 0.54502\n",
            "Epoch = 1501 RMSE Training Loss = 0.54502\n",
            "Epoch = 1502 RMSE Training Loss = 0.54502\n",
            "Epoch = 1503 RMSE Training Loss = 0.54502\n",
            "Epoch = 1504 RMSE Training Loss = 0.54502\n",
            "Epoch = 1505 RMSE Training Loss = 0.54502\n",
            "Epoch = 1506 RMSE Training Loss = 0.54502\n",
            "Epoch = 1507 RMSE Training Loss = 0.54503\n",
            "Epoch = 1508 RMSE Training Loss = 0.54503\n",
            "Epoch = 1509 RMSE Training Loss = 0.54503\n",
            "Epoch = 1510 RMSE Training Loss = 0.54503\n",
            "Epoch = 1511 RMSE Training Loss = 0.54503\n",
            "Epoch = 1512 RMSE Training Loss = 0.54503\n",
            "Epoch = 1513 RMSE Training Loss = 0.54503\n",
            "Epoch = 1514 RMSE Training Loss = 0.54503\n",
            "Epoch = 1515 RMSE Training Loss = 0.54503\n",
            "Epoch = 1516 RMSE Training Loss = 0.54503\n",
            "Epoch = 1517 RMSE Training Loss = 0.54503\n",
            "Epoch = 1518 RMSE Training Loss = 0.54503\n",
            "Epoch = 1519 RMSE Training Loss = 0.54504\n",
            "Epoch = 1520 RMSE Training Loss = 0.54504\n",
            "Epoch = 1521 RMSE Training Loss = 0.54504\n",
            "Epoch = 1522 RMSE Training Loss = 0.54504\n",
            "Epoch = 1523 RMSE Training Loss = 0.54504\n",
            "Epoch = 1524 RMSE Training Loss = 0.54504\n",
            "Epoch = 1525 RMSE Training Loss = 0.54504\n",
            "Epoch = 1526 RMSE Training Loss = 0.54504\n",
            "Epoch = 1527 RMSE Training Loss = 0.54504\n",
            "Epoch = 1528 RMSE Training Loss = 0.54504\n",
            "Epoch = 1529 RMSE Training Loss = 0.54504\n",
            "Epoch = 1530 RMSE Training Loss = 0.54504\n",
            "Epoch = 1531 RMSE Training Loss = 0.54505\n",
            "Epoch = 1532 RMSE Training Loss = 0.54505\n",
            "Epoch = 1533 RMSE Training Loss = 0.54505\n",
            "Epoch = 1534 RMSE Training Loss = 0.54505\n",
            "Epoch = 1535 RMSE Training Loss = 0.54505\n",
            "Epoch = 1536 RMSE Training Loss = 0.54505\n",
            "Epoch = 1537 RMSE Training Loss = 0.54505\n",
            "Epoch = 1538 RMSE Training Loss = 0.54505\n",
            "Epoch = 1539 RMSE Training Loss = 0.54505\n",
            "Epoch = 1540 RMSE Training Loss = 0.54505\n",
            "Epoch = 1541 RMSE Training Loss = 0.54505\n",
            "Epoch = 1542 RMSE Training Loss = 0.54505\n",
            "Epoch = 1543 RMSE Training Loss = 0.54505\n",
            "Epoch = 1544 RMSE Training Loss = 0.54505\n",
            "Epoch = 1545 RMSE Training Loss = 0.54505\n",
            "Epoch = 1546 RMSE Training Loss = 0.54506\n",
            "Epoch = 1547 RMSE Training Loss = 0.54506\n",
            "Epoch = 1548 RMSE Training Loss = 0.54506\n",
            "Epoch = 1549 RMSE Training Loss = 0.54506\n",
            "Epoch = 1550 RMSE Training Loss = 0.54506\n",
            "Epoch = 1551 RMSE Training Loss = 0.54506\n",
            "Epoch = 1552 RMSE Training Loss = 0.54506\n",
            "Epoch = 1553 RMSE Training Loss = 0.54506\n",
            "Epoch = 1554 RMSE Training Loss = 0.54506\n",
            "Epoch = 1555 RMSE Training Loss = 0.54506\n",
            "Epoch = 1556 RMSE Training Loss = 0.54506\n",
            "Epoch = 1557 RMSE Training Loss = 0.54506\n",
            "Epoch = 1558 RMSE Training Loss = 0.54506\n",
            "Epoch = 1559 RMSE Training Loss = 0.54506\n",
            "Epoch = 1560 RMSE Training Loss = 0.54506\n",
            "Epoch = 1561 RMSE Training Loss = 0.54506\n",
            "Epoch = 1562 RMSE Training Loss = 0.54507\n",
            "Epoch = 1563 RMSE Training Loss = 0.54507\n",
            "Epoch = 1564 RMSE Training Loss = 0.54507\n",
            "Epoch = 1565 RMSE Training Loss = 0.54507\n",
            "Epoch = 1566 RMSE Training Loss = 0.54507\n",
            "Epoch = 1567 RMSE Training Loss = 0.54507\n",
            "Epoch = 1568 RMSE Training Loss = 0.54507\n",
            "Epoch = 1569 RMSE Training Loss = 0.54507\n",
            "Epoch = 1570 RMSE Training Loss = 0.54507\n",
            "Epoch = 1571 RMSE Training Loss = 0.54507\n",
            "Epoch = 1572 RMSE Training Loss = 0.54507\n",
            "Epoch = 1573 RMSE Training Loss = 0.54507\n",
            "Epoch = 1574 RMSE Training Loss = 0.54507\n",
            "Epoch = 1575 RMSE Training Loss = 0.54507\n",
            "Epoch = 1576 RMSE Training Loss = 0.54507\n",
            "Epoch = 1577 RMSE Training Loss = 0.54507\n",
            "Epoch = 1578 RMSE Training Loss = 0.54507\n",
            "Epoch = 1579 RMSE Training Loss = 0.54507\n",
            "Epoch = 1580 RMSE Training Loss = 0.54507\n",
            "Epoch = 1581 RMSE Training Loss = 0.54507\n",
            "Epoch = 1582 RMSE Training Loss = 0.54507\n",
            "Epoch = 1583 RMSE Training Loss = 0.54508\n",
            "Epoch = 1584 RMSE Training Loss = 0.54508\n",
            "Epoch = 1585 RMSE Training Loss = 0.54508\n",
            "Epoch = 1586 RMSE Training Loss = 0.54508\n",
            "Epoch = 1587 RMSE Training Loss = 0.54508\n",
            "Epoch = 1588 RMSE Training Loss = 0.54508\n",
            "Epoch = 1589 RMSE Training Loss = 0.54508\n",
            "Epoch = 1590 RMSE Training Loss = 0.54508\n",
            "Epoch = 1591 RMSE Training Loss = 0.54508\n",
            "Epoch = 1592 RMSE Training Loss = 0.54508\n",
            "Epoch = 1593 RMSE Training Loss = 0.54508\n",
            "Epoch = 1594 RMSE Training Loss = 0.54508\n",
            "Epoch = 1595 RMSE Training Loss = 0.54508\n",
            "Epoch = 1596 RMSE Training Loss = 0.54508\n",
            "Epoch = 1597 RMSE Training Loss = 0.54508\n",
            "Epoch = 1598 RMSE Training Loss = 0.54508\n",
            "Epoch = 1599 RMSE Training Loss = 0.54508\n",
            "Epoch = 1600 RMSE Training Loss = 0.54508\n",
            "Epoch = 1601 RMSE Training Loss = 0.54508\n",
            "Epoch = 1602 RMSE Training Loss = 0.54508\n",
            "Epoch = 1603 RMSE Training Loss = 0.54508\n",
            "Epoch = 1604 RMSE Training Loss = 0.54508\n",
            "Epoch = 1605 RMSE Training Loss = 0.54508\n",
            "Epoch = 1606 RMSE Training Loss = 0.54508\n",
            "Epoch = 1607 RMSE Training Loss = 0.54508\n",
            "Epoch = 1608 RMSE Training Loss = 0.54508\n",
            "Epoch = 1609 RMSE Training Loss = 0.54509\n",
            "Epoch = 1610 RMSE Training Loss = 0.54509\n",
            "Epoch = 1611 RMSE Training Loss = 0.54509\n",
            "Epoch = 1612 RMSE Training Loss = 0.54509\n",
            "Epoch = 1613 RMSE Training Loss = 0.54509\n",
            "Epoch = 1614 RMSE Training Loss = 0.54509\n",
            "Epoch = 1615 RMSE Training Loss = 0.54509\n",
            "Epoch = 1616 RMSE Training Loss = 0.54509\n",
            "Epoch = 1617 RMSE Training Loss = 0.54509\n",
            "Epoch = 1618 RMSE Training Loss = 0.54509\n",
            "Epoch = 1619 RMSE Training Loss = 0.54509\n",
            "Epoch = 1620 RMSE Training Loss = 0.54509\n",
            "Epoch = 1621 RMSE Training Loss = 0.54509\n",
            "Epoch = 1622 RMSE Training Loss = 0.54509\n",
            "Epoch = 1623 RMSE Training Loss = 0.54509\n",
            "Epoch = 1624 RMSE Training Loss = 0.54509\n",
            "Epoch = 1625 RMSE Training Loss = 0.54509\n",
            "Epoch = 1626 RMSE Training Loss = 0.54509\n",
            "Epoch = 1627 RMSE Training Loss = 0.54509\n",
            "Epoch = 1628 RMSE Training Loss = 0.54509\n",
            "Epoch = 1629 RMSE Training Loss = 0.54509\n",
            "Epoch = 1630 RMSE Training Loss = 0.54509\n",
            "Epoch = 1631 RMSE Training Loss = 0.54509\n",
            "Epoch = 1632 RMSE Training Loss = 0.54509\n",
            "Epoch = 1633 RMSE Training Loss = 0.54509\n",
            "Epoch = 1634 RMSE Training Loss = 0.54509\n",
            "Epoch = 1635 RMSE Training Loss = 0.54509\n",
            "Epoch = 1636 RMSE Training Loss = 0.54509\n",
            "Epoch = 1637 RMSE Training Loss = 0.54509\n",
            "Epoch = 1638 RMSE Training Loss = 0.54509\n",
            "Epoch = 1639 RMSE Training Loss = 0.54509\n",
            "Epoch = 1640 RMSE Training Loss = 0.54509\n",
            "Epoch = 1641 RMSE Training Loss = 0.54509\n",
            "Epoch = 1642 RMSE Training Loss = 0.54509\n",
            "Epoch = 1643 RMSE Training Loss = 0.54509\n",
            "Epoch = 1644 RMSE Training Loss = 0.54509\n",
            "Epoch = 1645 RMSE Training Loss = 0.54509\n",
            "Epoch = 1646 RMSE Training Loss = 0.54509\n",
            "Epoch = 1647 RMSE Training Loss = 0.54509\n",
            "Epoch = 1648 RMSE Training Loss = 0.54509\n",
            "Epoch = 1649 RMSE Training Loss = 0.54509\n",
            "Epoch = 1650 RMSE Training Loss = 0.54509\n",
            "Epoch = 1651 RMSE Training Loss = 0.54509\n",
            "Epoch = 1652 RMSE Training Loss = 0.54509\n",
            "Epoch = 1653 RMSE Training Loss = 0.54509\n",
            "Epoch = 1654 RMSE Training Loss = 0.54509\n",
            "Epoch = 1655 RMSE Training Loss = 0.54509\n",
            "Epoch = 1656 RMSE Training Loss = 0.54509\n",
            "Epoch = 1657 RMSE Training Loss = 0.54509\n",
            "Epoch = 1658 RMSE Training Loss = 0.54509\n",
            "Epoch = 1659 RMSE Training Loss = 0.54509\n",
            "Epoch = 1660 RMSE Training Loss = 0.54509\n",
            "Epoch = 1661 RMSE Training Loss = 0.54509\n",
            "Epoch = 1662 RMSE Training Loss = 0.54510\n",
            "Epoch = 1663 RMSE Training Loss = 0.54510\n",
            "Epoch = 1664 RMSE Training Loss = 0.54510\n",
            "Epoch = 1665 RMSE Training Loss = 0.54510\n",
            "Epoch = 1666 RMSE Training Loss = 0.54510\n",
            "Epoch = 1667 RMSE Training Loss = 0.54510\n",
            "Epoch = 1668 RMSE Training Loss = 0.54509\n",
            "Epoch = 1669 RMSE Training Loss = 0.54510\n",
            "Epoch = 1670 RMSE Training Loss = 0.54510\n",
            "Epoch = 1671 RMSE Training Loss = 0.54510\n",
            "Epoch = 1672 RMSE Training Loss = 0.54510\n",
            "Epoch = 1673 RMSE Training Loss = 0.54510\n",
            "Epoch = 1674 RMSE Training Loss = 0.54510\n",
            "Epoch = 1675 RMSE Training Loss = 0.54510\n",
            "Epoch = 1676 RMSE Training Loss = 0.54510\n",
            "Epoch = 1677 RMSE Training Loss = 0.54510\n",
            "Epoch = 1678 RMSE Training Loss = 0.54510\n",
            "Epoch = 1679 RMSE Training Loss = 0.54510\n",
            "Epoch = 1680 RMSE Training Loss = 0.54510\n",
            "Epoch = 1681 RMSE Training Loss = 0.54510\n",
            "Epoch = 1682 RMSE Training Loss = 0.54510\n",
            "Epoch = 1683 RMSE Training Loss = 0.54510\n",
            "Epoch = 1684 RMSE Training Loss = 0.54510\n",
            "Epoch = 1685 RMSE Training Loss = 0.54509\n",
            "Epoch = 1686 RMSE Training Loss = 0.54509\n",
            "Epoch = 1687 RMSE Training Loss = 0.54509\n",
            "Epoch = 1688 RMSE Training Loss = 0.54509\n",
            "Epoch = 1689 RMSE Training Loss = 0.54509\n",
            "Epoch = 1690 RMSE Training Loss = 0.54509\n",
            "Epoch = 1691 RMSE Training Loss = 0.54509\n",
            "Epoch = 1692 RMSE Training Loss = 0.54509\n",
            "Epoch = 1693 RMSE Training Loss = 0.54509\n",
            "Epoch = 1694 RMSE Training Loss = 0.54509\n",
            "Epoch = 1695 RMSE Training Loss = 0.54509\n",
            "Epoch = 1696 RMSE Training Loss = 0.54509\n",
            "Epoch = 1697 RMSE Training Loss = 0.54509\n",
            "Epoch = 1698 RMSE Training Loss = 0.54509\n",
            "Epoch = 1699 RMSE Training Loss = 0.54509\n",
            "Epoch = 1700 RMSE Training Loss = 0.54509\n",
            "Epoch = 1701 RMSE Training Loss = 0.54509\n",
            "Epoch = 1702 RMSE Training Loss = 0.54509\n",
            "Epoch = 1703 RMSE Training Loss = 0.54509\n",
            "Epoch = 1704 RMSE Training Loss = 0.54509\n",
            "Epoch = 1705 RMSE Training Loss = 0.54509\n",
            "Epoch = 1706 RMSE Training Loss = 0.54509\n",
            "Epoch = 1707 RMSE Training Loss = 0.54509\n",
            "Epoch = 1708 RMSE Training Loss = 0.54509\n",
            "Epoch = 1709 RMSE Training Loss = 0.54509\n",
            "Epoch = 1710 RMSE Training Loss = 0.54509\n",
            "Epoch = 1711 RMSE Training Loss = 0.54509\n",
            "Epoch = 1712 RMSE Training Loss = 0.54509\n",
            "Epoch = 1713 RMSE Training Loss = 0.54509\n",
            "Epoch = 1714 RMSE Training Loss = 0.54509\n",
            "Epoch = 1715 RMSE Training Loss = 0.54509\n",
            "Epoch = 1716 RMSE Training Loss = 0.54509\n",
            "Epoch = 1717 RMSE Training Loss = 0.54509\n",
            "Epoch = 1718 RMSE Training Loss = 0.54509\n",
            "Epoch = 1719 RMSE Training Loss = 0.54509\n",
            "Epoch = 1720 RMSE Training Loss = 0.54509\n",
            "Epoch = 1721 RMSE Training Loss = 0.54509\n",
            "Epoch = 1722 RMSE Training Loss = 0.54509\n",
            "Epoch = 1723 RMSE Training Loss = 0.54509\n",
            "Epoch = 1724 RMSE Training Loss = 0.54509\n",
            "Epoch = 1725 RMSE Training Loss = 0.54509\n",
            "Epoch = 1726 RMSE Training Loss = 0.54509\n",
            "Epoch = 1727 RMSE Training Loss = 0.54509\n",
            "Epoch = 1728 RMSE Training Loss = 0.54509\n",
            "Epoch = 1729 RMSE Training Loss = 0.54509\n",
            "Epoch = 1730 RMSE Training Loss = 0.54509\n",
            "Epoch = 1731 RMSE Training Loss = 0.54509\n",
            "Epoch = 1732 RMSE Training Loss = 0.54509\n",
            "Epoch = 1733 RMSE Training Loss = 0.54509\n",
            "Epoch = 1734 RMSE Training Loss = 0.54509\n",
            "Epoch = 1735 RMSE Training Loss = 0.54509\n",
            "Epoch = 1736 RMSE Training Loss = 0.54509\n",
            "Epoch = 1737 RMSE Training Loss = 0.54509\n",
            "Epoch = 1738 RMSE Training Loss = 0.54509\n",
            "Epoch = 1739 RMSE Training Loss = 0.54509\n",
            "Epoch = 1740 RMSE Training Loss = 0.54509\n",
            "Epoch = 1741 RMSE Training Loss = 0.54509\n",
            "Epoch = 1742 RMSE Training Loss = 0.54509\n",
            "Epoch = 1743 RMSE Training Loss = 0.54508\n",
            "Epoch = 1744 RMSE Training Loss = 0.54508\n",
            "Epoch = 1745 RMSE Training Loss = 0.54508\n",
            "Epoch = 1746 RMSE Training Loss = 0.54508\n",
            "Epoch = 1747 RMSE Training Loss = 0.54508\n",
            "Epoch = 1748 RMSE Training Loss = 0.54508\n",
            "Epoch = 1749 RMSE Training Loss = 0.54508\n",
            "Epoch = 1750 RMSE Training Loss = 0.54508\n",
            "Epoch = 1751 RMSE Training Loss = 0.54508\n",
            "Epoch = 1752 RMSE Training Loss = 0.54508\n",
            "Epoch = 1753 RMSE Training Loss = 0.54508\n",
            "Epoch = 1754 RMSE Training Loss = 0.54508\n",
            "Epoch = 1755 RMSE Training Loss = 0.54508\n",
            "Epoch = 1756 RMSE Training Loss = 0.54508\n",
            "Epoch = 1757 RMSE Training Loss = 0.54508\n",
            "Epoch = 1758 RMSE Training Loss = 0.54508\n",
            "Epoch = 1759 RMSE Training Loss = 0.54508\n",
            "Epoch = 1760 RMSE Training Loss = 0.54508\n",
            "Epoch = 1761 RMSE Training Loss = 0.54508\n",
            "Epoch = 1762 RMSE Training Loss = 0.54508\n",
            "Epoch = 1763 RMSE Training Loss = 0.54508\n",
            "Epoch = 1764 RMSE Training Loss = 0.54508\n",
            "Epoch = 1765 RMSE Training Loss = 0.54508\n",
            "Epoch = 1766 RMSE Training Loss = 0.54508\n",
            "Epoch = 1767 RMSE Training Loss = 0.54508\n",
            "Epoch = 1768 RMSE Training Loss = 0.54508\n",
            "Epoch = 1769 RMSE Training Loss = 0.54508\n",
            "Epoch = 1770 RMSE Training Loss = 0.54507\n",
            "Epoch = 1771 RMSE Training Loss = 0.54507\n",
            "Epoch = 1772 RMSE Training Loss = 0.54507\n",
            "Epoch = 1773 RMSE Training Loss = 0.54507\n",
            "Epoch = 1774 RMSE Training Loss = 0.54507\n",
            "Epoch = 1775 RMSE Training Loss = 0.54507\n",
            "Epoch = 1776 RMSE Training Loss = 0.54507\n",
            "Epoch = 1777 RMSE Training Loss = 0.54507\n",
            "Epoch = 1778 RMSE Training Loss = 0.54507\n",
            "Epoch = 1779 RMSE Training Loss = 0.54507\n",
            "Epoch = 1780 RMSE Training Loss = 0.54507\n",
            "Epoch = 1781 RMSE Training Loss = 0.54507\n",
            "Epoch = 1782 RMSE Training Loss = 0.54507\n",
            "Epoch = 1783 RMSE Training Loss = 0.54507\n",
            "Epoch = 1784 RMSE Training Loss = 0.54507\n",
            "Epoch = 1785 RMSE Training Loss = 0.54507\n",
            "Epoch = 1786 RMSE Training Loss = 0.54507\n",
            "Epoch = 1787 RMSE Training Loss = 0.54507\n",
            "Epoch = 1788 RMSE Training Loss = 0.54507\n",
            "Epoch = 1789 RMSE Training Loss = 0.54507\n",
            "Epoch = 1790 RMSE Training Loss = 0.54507\n",
            "Epoch = 1791 RMSE Training Loss = 0.54506\n",
            "Epoch = 1792 RMSE Training Loss = 0.54506\n",
            "Epoch = 1793 RMSE Training Loss = 0.54506\n",
            "Epoch = 1794 RMSE Training Loss = 0.54506\n",
            "Epoch = 1795 RMSE Training Loss = 0.54506\n",
            "Epoch = 1796 RMSE Training Loss = 0.54506\n",
            "Epoch = 1797 RMSE Training Loss = 0.54506\n",
            "Epoch = 1798 RMSE Training Loss = 0.54506\n",
            "Epoch = 1799 RMSE Training Loss = 0.54506\n",
            "Epoch = 1800 RMSE Training Loss = 0.54506\n",
            "Epoch = 1801 RMSE Training Loss = 0.54506\n",
            "Epoch = 1802 RMSE Training Loss = 0.54506\n",
            "Epoch = 1803 RMSE Training Loss = 0.54506\n",
            "Epoch = 1804 RMSE Training Loss = 0.54506\n",
            "Epoch = 1805 RMSE Training Loss = 0.54506\n",
            "Epoch = 1806 RMSE Training Loss = 0.54506\n",
            "Epoch = 1807 RMSE Training Loss = 0.54506\n",
            "Epoch = 1808 RMSE Training Loss = 0.54505\n",
            "Epoch = 1809 RMSE Training Loss = 0.54505\n",
            "Epoch = 1810 RMSE Training Loss = 0.54505\n",
            "Epoch = 1811 RMSE Training Loss = 0.54505\n",
            "Epoch = 1812 RMSE Training Loss = 0.54505\n",
            "Epoch = 1813 RMSE Training Loss = 0.54505\n",
            "Epoch = 1814 RMSE Training Loss = 0.54505\n",
            "Epoch = 1815 RMSE Training Loss = 0.54505\n",
            "Epoch = 1816 RMSE Training Loss = 0.54505\n",
            "Epoch = 1817 RMSE Training Loss = 0.54505\n",
            "Epoch = 1818 RMSE Training Loss = 0.54505\n",
            "Epoch = 1819 RMSE Training Loss = 0.54505\n",
            "Epoch = 1820 RMSE Training Loss = 0.54505\n",
            "Epoch = 1821 RMSE Training Loss = 0.54505\n",
            "Epoch = 1822 RMSE Training Loss = 0.54505\n",
            "Epoch = 1823 RMSE Training Loss = 0.54505\n",
            "Epoch = 1824 RMSE Training Loss = 0.54504\n",
            "Epoch = 1825 RMSE Training Loss = 0.54504\n",
            "Epoch = 1826 RMSE Training Loss = 0.54504\n",
            "Epoch = 1827 RMSE Training Loss = 0.54504\n",
            "Epoch = 1828 RMSE Training Loss = 0.54504\n",
            "Epoch = 1829 RMSE Training Loss = 0.54504\n",
            "Epoch = 1830 RMSE Training Loss = 0.54504\n",
            "Epoch = 1831 RMSE Training Loss = 0.54504\n",
            "Epoch = 1832 RMSE Training Loss = 0.54504\n",
            "Epoch = 1833 RMSE Training Loss = 0.54504\n",
            "Epoch = 1834 RMSE Training Loss = 0.54504\n",
            "Epoch = 1835 RMSE Training Loss = 0.54504\n",
            "Epoch = 1836 RMSE Training Loss = 0.54504\n",
            "Epoch = 1837 RMSE Training Loss = 0.54504\n",
            "Epoch = 1838 RMSE Training Loss = 0.54503\n",
            "Epoch = 1839 RMSE Training Loss = 0.54503\n",
            "Epoch = 1840 RMSE Training Loss = 0.54503\n",
            "Epoch = 1841 RMSE Training Loss = 0.54503\n",
            "Epoch = 1842 RMSE Training Loss = 0.54503\n",
            "Epoch = 1843 RMSE Training Loss = 0.54503\n",
            "Epoch = 1844 RMSE Training Loss = 0.54503\n",
            "Epoch = 1845 RMSE Training Loss = 0.54503\n",
            "Epoch = 1846 RMSE Training Loss = 0.54503\n",
            "Epoch = 1847 RMSE Training Loss = 0.54503\n",
            "Epoch = 1848 RMSE Training Loss = 0.54503\n",
            "Epoch = 1849 RMSE Training Loss = 0.54503\n",
            "Epoch = 1850 RMSE Training Loss = 0.54502\n",
            "Epoch = 1851 RMSE Training Loss = 0.54502\n",
            "Epoch = 1852 RMSE Training Loss = 0.54502\n",
            "Epoch = 1853 RMSE Training Loss = 0.54502\n",
            "Epoch = 1854 RMSE Training Loss = 0.54502\n",
            "Epoch = 1855 RMSE Training Loss = 0.54502\n",
            "Epoch = 1856 RMSE Training Loss = 0.54502\n",
            "Epoch = 1857 RMSE Training Loss = 0.54502\n",
            "Epoch = 1858 RMSE Training Loss = 0.54502\n",
            "Epoch = 1859 RMSE Training Loss = 0.54502\n",
            "Epoch = 1860 RMSE Training Loss = 0.54502\n",
            "Epoch = 1861 RMSE Training Loss = 0.54502\n",
            "Epoch = 1862 RMSE Training Loss = 0.54501\n",
            "Epoch = 1863 RMSE Training Loss = 0.54501\n",
            "Epoch = 1864 RMSE Training Loss = 0.54501\n",
            "Epoch = 1865 RMSE Training Loss = 0.54501\n",
            "Epoch = 1866 RMSE Training Loss = 0.54501\n",
            "Epoch = 1867 RMSE Training Loss = 0.54501\n",
            "Epoch = 1868 RMSE Training Loss = 0.54501\n",
            "Epoch = 1869 RMSE Training Loss = 0.54501\n",
            "Epoch = 1870 RMSE Training Loss = 0.54501\n",
            "Epoch = 1871 RMSE Training Loss = 0.54501\n",
            "Epoch = 1872 RMSE Training Loss = 0.54501\n",
            "Epoch = 1873 RMSE Training Loss = 0.54500\n",
            "Epoch = 1874 RMSE Training Loss = 0.54500\n",
            "Epoch = 1875 RMSE Training Loss = 0.54500\n",
            "Epoch = 1876 RMSE Training Loss = 0.54500\n",
            "Epoch = 1877 RMSE Training Loss = 0.54500\n",
            "Epoch = 1878 RMSE Training Loss = 0.54500\n",
            "Epoch = 1879 RMSE Training Loss = 0.54500\n",
            "Epoch = 1880 RMSE Training Loss = 0.54500\n",
            "Epoch = 1881 RMSE Training Loss = 0.54500\n",
            "Epoch = 1882 RMSE Training Loss = 0.54500\n",
            "Epoch = 1883 RMSE Training Loss = 0.54500\n",
            "Epoch = 1884 RMSE Training Loss = 0.54499\n",
            "Epoch = 1885 RMSE Training Loss = 0.54499\n",
            "Epoch = 1886 RMSE Training Loss = 0.54499\n",
            "Epoch = 1887 RMSE Training Loss = 0.54499\n",
            "Epoch = 1888 RMSE Training Loss = 0.54499\n",
            "Epoch = 1889 RMSE Training Loss = 0.54499\n",
            "Epoch = 1890 RMSE Training Loss = 0.54499\n",
            "Epoch = 1891 RMSE Training Loss = 0.54499\n",
            "Epoch = 1892 RMSE Training Loss = 0.54499\n",
            "Epoch = 1893 RMSE Training Loss = 0.54499\n",
            "Epoch = 1894 RMSE Training Loss = 0.54498\n",
            "Epoch = 1895 RMSE Training Loss = 0.54498\n",
            "Epoch = 1896 RMSE Training Loss = 0.54498\n",
            "Epoch = 1897 RMSE Training Loss = 0.54498\n",
            "Epoch = 1898 RMSE Training Loss = 0.54498\n",
            "Epoch = 1899 RMSE Training Loss = 0.54498\n",
            "Epoch = 1900 RMSE Training Loss = 0.54498\n",
            "Epoch = 1901 RMSE Training Loss = 0.54498\n",
            "Epoch = 1902 RMSE Training Loss = 0.54498\n",
            "Epoch = 1903 RMSE Training Loss = 0.54497\n",
            "Epoch = 1904 RMSE Training Loss = 0.54497\n",
            "Epoch = 1905 RMSE Training Loss = 0.54497\n",
            "Epoch = 1906 RMSE Training Loss = 0.54497\n",
            "Epoch = 1907 RMSE Training Loss = 0.54497\n",
            "Epoch = 1908 RMSE Training Loss = 0.54497\n",
            "Epoch = 1909 RMSE Training Loss = 0.54497\n",
            "Epoch = 1910 RMSE Training Loss = 0.54497\n",
            "Epoch = 1911 RMSE Training Loss = 0.54497\n",
            "Epoch = 1912 RMSE Training Loss = 0.54497\n",
            "Epoch = 1913 RMSE Training Loss = 0.54496\n",
            "Epoch = 1914 RMSE Training Loss = 0.54496\n",
            "Epoch = 1915 RMSE Training Loss = 0.54496\n",
            "Epoch = 1916 RMSE Training Loss = 0.54496\n",
            "Epoch = 1917 RMSE Training Loss = 0.54496\n",
            "Epoch = 1918 RMSE Training Loss = 0.54496\n",
            "Epoch = 1919 RMSE Training Loss = 0.54496\n",
            "Epoch = 1920 RMSE Training Loss = 0.54496\n",
            "Epoch = 1921 RMSE Training Loss = 0.54496\n",
            "Epoch = 1922 RMSE Training Loss = 0.54495\n",
            "Epoch = 1923 RMSE Training Loss = 0.54495\n",
            "Epoch = 1924 RMSE Training Loss = 0.54495\n",
            "Epoch = 1925 RMSE Training Loss = 0.54495\n",
            "Epoch = 1926 RMSE Training Loss = 0.54495\n",
            "Epoch = 1927 RMSE Training Loss = 0.54495\n",
            "Epoch = 1928 RMSE Training Loss = 0.54495\n",
            "Epoch = 1929 RMSE Training Loss = 0.54495\n",
            "Epoch = 1930 RMSE Training Loss = 0.54495\n",
            "Epoch = 1931 RMSE Training Loss = 0.54494\n",
            "Epoch = 1932 RMSE Training Loss = 0.54494\n",
            "Epoch = 1933 RMSE Training Loss = 0.54494\n",
            "Epoch = 1934 RMSE Training Loss = 0.54494\n",
            "Epoch = 1935 RMSE Training Loss = 0.54494\n",
            "Epoch = 1936 RMSE Training Loss = 0.54494\n",
            "Epoch = 1937 RMSE Training Loss = 0.54494\n",
            "Epoch = 1938 RMSE Training Loss = 0.54494\n",
            "Epoch = 1939 RMSE Training Loss = 0.54493\n",
            "Epoch = 1940 RMSE Training Loss = 0.54493\n",
            "Epoch = 1941 RMSE Training Loss = 0.54493\n",
            "Epoch = 1942 RMSE Training Loss = 0.54493\n",
            "Epoch = 1943 RMSE Training Loss = 0.54493\n",
            "Epoch = 1944 RMSE Training Loss = 0.54493\n",
            "Epoch = 1945 RMSE Training Loss = 0.54493\n",
            "Epoch = 1946 RMSE Training Loss = 0.54493\n",
            "Epoch = 1947 RMSE Training Loss = 0.54492\n",
            "Epoch = 1948 RMSE Training Loss = 0.54492\n",
            "Epoch = 1949 RMSE Training Loss = 0.54492\n",
            "Epoch = 1950 RMSE Training Loss = 0.54492\n",
            "Epoch = 1951 RMSE Training Loss = 0.54492\n",
            "Epoch = 1952 RMSE Training Loss = 0.54492\n",
            "Epoch = 1953 RMSE Training Loss = 0.54492\n",
            "Epoch = 1954 RMSE Training Loss = 0.54492\n",
            "Epoch = 1955 RMSE Training Loss = 0.54491\n",
            "Epoch = 1956 RMSE Training Loss = 0.54491\n",
            "Epoch = 1957 RMSE Training Loss = 0.54491\n",
            "Epoch = 1958 RMSE Training Loss = 0.54491\n",
            "Epoch = 1959 RMSE Training Loss = 0.54491\n",
            "Epoch = 1960 RMSE Training Loss = 0.54491\n",
            "Epoch = 1961 RMSE Training Loss = 0.54491\n",
            "Epoch = 1962 RMSE Training Loss = 0.54491\n",
            "Epoch = 1963 RMSE Training Loss = 0.54490\n",
            "Epoch = 1964 RMSE Training Loss = 0.54490\n",
            "Epoch = 1965 RMSE Training Loss = 0.54490\n",
            "Epoch = 1966 RMSE Training Loss = 0.54490\n",
            "Epoch = 1967 RMSE Training Loss = 0.54490\n",
            "Epoch = 1968 RMSE Training Loss = 0.54490\n",
            "Epoch = 1969 RMSE Training Loss = 0.54490\n",
            "Epoch = 1970 RMSE Training Loss = 0.54489\n",
            "Epoch = 1971 RMSE Training Loss = 0.54489\n",
            "Epoch = 1972 RMSE Training Loss = 0.54489\n",
            "Epoch = 1973 RMSE Training Loss = 0.54489\n",
            "Epoch = 1974 RMSE Training Loss = 0.54489\n",
            "Epoch = 1975 RMSE Training Loss = 0.54489\n",
            "Epoch = 1976 RMSE Training Loss = 0.54489\n",
            "Epoch = 1977 RMSE Training Loss = 0.54489\n",
            "Epoch = 1978 RMSE Training Loss = 0.54488\n",
            "Epoch = 1979 RMSE Training Loss = 0.54488\n",
            "Epoch = 1980 RMSE Training Loss = 0.54488\n",
            "Epoch = 1981 RMSE Training Loss = 0.54488\n",
            "Epoch = 1982 RMSE Training Loss = 0.54488\n",
            "Epoch = 1983 RMSE Training Loss = 0.54488\n",
            "Epoch = 1984 RMSE Training Loss = 0.54488\n",
            "Epoch = 1985 RMSE Training Loss = 0.54487\n",
            "Epoch = 1986 RMSE Training Loss = 0.54487\n",
            "Epoch = 1987 RMSE Training Loss = 0.54487\n",
            "Epoch = 1988 RMSE Training Loss = 0.54487\n",
            "Epoch = 1989 RMSE Training Loss = 0.54487\n",
            "Epoch = 1990 RMSE Training Loss = 0.54487\n",
            "Epoch = 1991 RMSE Training Loss = 0.54487\n",
            "Epoch = 1992 RMSE Training Loss = 0.54486\n",
            "Epoch = 1993 RMSE Training Loss = 0.54486\n",
            "Epoch = 1994 RMSE Training Loss = 0.54486\n",
            "Epoch = 1995 RMSE Training Loss = 0.54486\n",
            "Epoch = 1996 RMSE Training Loss = 0.54486\n",
            "Epoch = 1997 RMSE Training Loss = 0.54486\n",
            "Epoch = 1998 RMSE Training Loss = 0.54486\n",
            "Epoch = 1999 RMSE Training Loss = 0.54485\n",
            "Epoch = 2000 RMSE Training Loss = 0.54485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fvvSXJBkvQp"
      },
      "source": [
        "# Evaluating the Model Using Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvMOp1sCwAtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffd060e-170a-450b-dc80-25f666b96f19"
      },
      "source": [
        "net.eval()\n",
        "with torch.set_grad_enabled(False):\n",
        "  calculate_scores(net,test_dataloader)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_RMSE_loss: 0.62001\n",
            "total_mse_loss: 0.38441\n",
            "total_mae_loss: 0.50692\n",
            "total_mape_loss: 0.06326\n",
            "total_r_square:0.92556\n",
            "total_esd: 0.35698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0JmAvh4kGXb"
      },
      "source": [
        "# -----------------------------------------------------------------------END--------------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    }
  ]
}